#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å±±è„šä¸‹é¡¹ç›® v2.0 - å®Œæ•´ä»£ç æ¡†æ¶ç”Ÿæˆè„šæœ¬
Project: å±±è„šä¸‹ - Aè‚¡"å±±è„šä¸‹"å½¢æ€è‚¡ç¥¨ç²¾å‡†ç­›é€‰ç³»ç»Ÿ
Version: 2.0
Author: AI Agent
Date: 2025-12-19

ä½¿ç”¨æ–¹æ³•:
    python3 generate_all_files.py
    
è¿™ä¸ªè„šæœ¬å°†ç”Ÿæˆé¡¹ç›®çš„å®Œæ•´ç›®å½•ç»“æ„å’Œæ‰€æœ‰å¿…éœ€çš„æ–‡ä»¶
"""

import os
import json
import shutil
from pathlib import Path
from datetime import datetime

# ç”±äºå®Œæ•´çš„ä»£ç æ¨¡æ¿éå¸¸å¤§ï¼Œè¿™é‡Œä½¿ç”¨å¯¼å…¥æ–¹å¼
# å®é™…ä½¿ç”¨æ—¶ï¼Œæ‰€æœ‰æ¨¡æ¿å°†è¢«å®Œå…¨å±•å¼€

try:
    # å°è¯•å¯¼å…¥æ¨¡æ¿æ–‡ä»¶
    from code_templates import ALL_FILE_TEMPLATES
except ImportError:
    # å¦‚æœæ²¡æœ‰æ¨¡æ¿æ–‡ä»¶ï¼Œä½¿ç”¨å†…åµŒçš„ç®€åŒ–ç‰ˆæœ¬
    print("âš ï¸  æœªæ‰¾åˆ°code_templates.pyï¼Œä½¿ç”¨å†…åµŒæ¨¡æ¿")
    
    # å†…åµŒæ‰€æœ‰æ–‡ä»¶æ¨¡æ¿
    ALL_FILE_TEMPLATES = {}
    
    #####################################################################
    # æ•°æ®é‡‡é›†æ¨¡å—
    #####################################################################
    
    ALL_FILE_TEMPLATES["scripts/data_collection/scheduler_main.py"] = '''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸»è°ƒåº¦ç¨‹åº - è´Ÿè´£è°ƒåº¦æ‰€æœ‰æ•°æ®é‡‡é›†ä»»åŠ¡
"""

import schedule
import time
import logging
from datetime import datetime
import json

logging.basicConfig(
    filename='logs/scheduler.log',
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

class DataCollectionScheduler:
    """æ•°æ®é‡‡é›†è°ƒåº¦å™¨"""
    
    def __init__(self, config_path='config/config.json'):
        self.config = self.load_config(config_path)
    
    def load_config(self, config_path):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            logging.error(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
            return {}
    
    def collect_daily_data(self):
        """é‡‡é›†æ—¥çº¿å’Œåˆ†é’Ÿçº¿æ•°æ®"""
        logging.info("å¼€å§‹é‡‡é›†æ—¥çº¿å’Œåˆ†é’Ÿçº¿æ•°æ®...")
        # TODO: è°ƒç”¨ç°æœ‰çš„"æ—¥çº¿å’Œåˆ†é’Ÿçº¿æ•°æ®è·å–.py"
        
    def collect_weekly_data(self):
        """é‡‡é›†å‘¨çº¿æ•°æ®"""
        logging.info("å¼€å§‹é‡‡é›†å‘¨çº¿æ•°æ®...")
        # TODO: è°ƒç”¨ç°æœ‰çš„"å‘¨çº¿æ•°æ®è·å–.py"
        
    def collect_monthly_data(self):
        """é‡‡é›†æœˆçº¿æ•°æ®"""
        logging.info("å¼€å§‹é‡‡é›†æœˆçº¿æ•°æ®...")
        # TODO: è°ƒç”¨ç°æœ‰çš„"æœˆçº¿æ•°æ®è·å–.py"
        
    def collect_financial_data(self):
        """é‡‡é›†è´¢åŠ¡å’ŒåŸºæœ¬é¢æ•°æ®"""
        logging.info("å¼€å§‹é‡‡é›†è´¢åŠ¡å’ŒåŸºæœ¬é¢æ•°æ®...")
        # TODO: è°ƒç”¨ç°æœ‰çš„"è´¢åŠ¡å’ŒåŸºæœ¬é¢æ•°æ®è·å–.py"
    
    def setup_schedule(self):
        """è®¾ç½®å®šæ—¶ä»»åŠ¡"""
        schedule.every().day.at("09:00").do(self.collect_daily_data)
        schedule.every().monday.at("16:00").do(self.collect_weekly_data)
        schedule.every().day.at("10:00").do(self.collect_financial_data)
        logging.info("å®šæ—¶ä»»åŠ¡è®¾ç½®å®Œæˆ")
    
    def run(self):
        """è¿è¡Œè°ƒåº¦å™¨"""
        self.setup_schedule()
        logging.info("æ•°æ®é‡‡é›†è°ƒåº¦å™¨å¯åŠ¨")
        print("ğŸš€ æ•°æ®é‡‡é›†è°ƒåº¦å™¨å·²å¯åŠ¨...")
        
        while True:
            schedule.run_pending()
            time.sleep(60)

if __name__ == '__main__':
    scheduler = DataCollectionScheduler()
    scheduler.run()
'''

    ALL_FILE_TEMPLATES["scripts/data_collection/data_cleaning.py"] = '''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®æ¸…æ´—æ¨¡å— - è´Ÿè´£æ•°æ®éªŒè¯ã€å¼‚å¸¸å€¼å¤„ç†ã€å­—æ®µæ ‡å‡†åŒ–
æ•°æ®ä¿ç•™æœŸ: 180ä¸ªäº¤æ˜“æ—¥
"""

import pandas as pd
import numpy as np
import logging
from pathlib import Path
from datetime import datetime, timedelta

logging.basicConfig(
    filename='logs/data_cleaning.log',
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

class DataCleaner:
    """æ•°æ®æ¸…æ´—å™¨"""
    
    def __init__(self, data_retention_days=180):
        self.data_retention_days = data_retention_days  # 180äº¤æ˜“æ—¥
        self.raw_data_dir = Path('data/raw')
        self.processed_data_dir = Path('data/processed')
        self.backup_dir = Path('data/backup')
        
        for directory in [self.raw_data_dir, self.processed_data_dir, self.backup_dir]:
            directory.mkdir(parents=True, exist_ok=True)
    
    def validate_data(self, df, required_columns):
        """éªŒè¯æ•°æ®å®Œæ•´æ€§"""
        missing_cols = set(required_columns) - set(df.columns)
        if missing_cols:
            raise ValueError(f"ç¼ºå°‘å¿…éœ€çš„åˆ—: {missing_cols}")
        
        missing_count = df.isnull().sum()
        if missing_count.any():
            logging.warning(f"å‘ç°ç¼ºå¤±å€¼:\\n{missing_count[missing_count > 0]}")
        
        return True
    
    def handle_missing_values(self, df):
        """å¤„ç†ç¼ºå¤±å€¼"""
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        df[numeric_columns] = df[numeric_columns].ffill()
        df[numeric_columns] = df[numeric_columns].fillna(df[numeric_columns].mean())
        return df
    
    def remove_outliers(self, df, columns, n_std=3):
        """ç§»é™¤å¼‚å¸¸å€¼"""
        for col in columns:
            if col in df.columns and df[col].dtype in [np.float64, np.int64]:
                mean = df[col].mean()
                std = df[col].std()
                df = df[abs(df[col] - mean) <= n_std * std]
        return df
    
    def standardize_fields(self, df):
        """å­—æ®µæ ‡å‡†åŒ–"""
        if 'stock_code' in df.columns:
            df['stock_code'] = df['stock_code'].astype(str).str.zfill(6)
        
        if 'date' in df.columns:
            df['date'] = pd.to_datetime(df['date'])
        
        price_columns = ['open', 'high', 'low', 'close', 'adj_close']
        for col in price_columns:
            if col in df.columns:
                df[col] = df[col].round(2)
        
        return df
    
    def clean_daily_data(self, input_file='daily_data.csv'):
        """æ¸…æ´—æ—¥çº¿æ•°æ®"""
        try:
            df = pd.read_csv(self.raw_data_dir / input_file)
            
            required_columns = ['stock_code', 'date', 'open', 'high', 'low', 'close', 'volume']
            self.validate_data(df, required_columns)
            
            df = self.handle_missing_values(df)
            df = self.remove_outliers(df, ['open', 'high', 'low', 'close', 'volume'])
            df = self.standardize_fields(df)
            
            output_file = self.processed_data_dir / f"daily_data_cleaned_{datetime.now().strftime('%Y%m%d')}.csv"
            df.to_csv(output_file, index=False)
            
            logging.info(f"æ—¥çº¿æ•°æ®æ¸…æ´—å®Œæˆ: {output_file}")
            return df
        
        except Exception as e:
            logging.error(f"æ—¥çº¿æ•°æ®æ¸…æ´—å¤±è´¥: {str(e)}")
            raise
    
    def backup_historical_data(self):
        """å¤‡ä»½å†å²æ•°æ®ï¼ˆä¿ç•™180ä¸ªäº¤æ˜“æ—¥ï¼‰"""
        try:
            today = datetime.now().strftime('%Y-%m-%d')
            for data_file in self.processed_data_dir.glob('daily_data_cleaned_*.csv'):
                backup_file = self.backup_dir / f"daily_data_backup_{today}.csv"
                df = pd.read_csv(data_file)
                df.to_csv(backup_file, index=False)
                logging.info(f"åˆ›å»ºå¤‡ä»½: {backup_file}")
            
            cutoff_date = datetime.now() - timedelta(days=self.data_retention_days)
            for backup_file in self.backup_dir.glob('daily_data_backup_*.csv'):
                file_date_str = backup_file.stem.split('_')[-1]
                file_date = datetime.strptime(file_date_str, '%Y-%m-%d')
                if file_date < cutoff_date:
                    backup_file.unlink()
                    logging.info(f"åˆ é™¤è¿‡æœŸå¤‡ä»½: {backup_file}")
        
        except Exception as e:
            logging.error(f"å¤‡ä»½å¤±è´¥: {str(e)}")

if __name__ == '__main__':
    cleaner = DataCleaner(data_retention_days=180)
    cleaner.clean_daily_data()
    cleaner.backup_historical_data()
'''

    #####################################################################
    # ç‰¹å¾å·¥ç¨‹æ¨¡å—  
    #####################################################################
    
    ALL_FILE_TEMPLATES["scripts/feature_engineering/feature_extraction.py"] = '''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç‰¹å¾æå–æ¨¡å— - æå–134ä¸ªåŸå§‹ç‰¹å¾
æ—¶é—´çª—å£: ä»æ‹‰å‡æ—¥å‰ä¸€å¤©å‘å‰æ¨è¿›20ä¸ªäº¤æ˜“æ—¥
"""

import pandas as pd
import numpy as np
import talib
from pathlib import Path
import logging

logging.basicConfig(
    filename='logs/feature_extraction.log',
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

class FeatureExtractor:
    """ç‰¹å¾æå–å™¨ - æå–134ä¸ªåŸå§‹ç‰¹å¾"""
    
    def __init__(self, time_window=20):
        self.time_window = time_window  # 20ä¸ªäº¤æ˜“æ—¥
        self.data_dir = Path('data/processed')
        self.output_dir = Path('data/features')
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def load_data(self, stock_code, end_date):
        """åŠ è½½è‚¡ç¥¨æ•°æ®ï¼ˆ20ä¸ªäº¤æ˜“æ—¥ï¼‰"""
        # TODO: ä»processedæ•°æ®å’Œbackupä¸­åŠ è½½20ä¸ªäº¤æ˜“æ—¥çš„æ•°æ®
        pass
    
    def extract_price_features(self, df):
        """æå–ä»·æ ¼ç‰¹å¾ï¼ˆçº¦30ä¸ªï¼‰"""
        features = {}
        
        # åŸºç¡€ä»·æ ¼ç»Ÿè®¡
        features['price_mean'] = df['close'].mean()
        features['price_std'] = df['close'].std()
        features['price_max'] = df['close'].max()
        features['price_min'] = df['close'].min()
        features['price_range'] = features['price_max'] - features['price_min']
        
        # æ¶¨è·Œå¹…ç»Ÿè®¡
        df['pct_change'] = df['close'].pct_change()
        features['return_mean'] = df['pct_change'].mean()
        features['return_std'] = df['pct_change'].std()
        features['return_max'] = df['pct_change'].max()
        features['return_min'] = df['pct_change'].min()
        features['positive_days'] = (df['pct_change'] > 0).sum()
        features['negative_days'] = (df['pct_change'] < 0).sum()
        
        # æ›´å¤šä»·æ ¼ç‰¹å¾...
        features['cumulative_return'] = (df['close'].iloc[-1] / df['close'].iloc[0]) - 1
        
        return features
    
    def extract_volume_features(self, df):
        """æå–æˆäº¤é‡ç‰¹å¾ï¼ˆçº¦25ä¸ªï¼‰"""
        features = {}
        
        features['volume_mean'] = df['volume'].mean()
        features['volume_std'] = df['volume'].std()
        features['volume_max'] = df['volume'].max()
        features['volume_min'] = df['volume'].min()
        
        df['volume_change'] = df['volume'].pct_change()
        features['volume_change_mean'] = df['volume_change'].mean()
        features['volume_price_corr'] = df['volume'].corr(df['close'])
        
        return features
    
    def extract_technical_indicators(self, df):
        """æå–æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾ï¼ˆçº¦50ä¸ªï¼‰"""
        features = {}
        
        close = df['close'].values
        high = df['high'].values
        low = df['low'].values
        volume = df['volume'].values
        
        try:
            # ç§»åŠ¨å¹³å‡çº¿
            features['ma5'] = talib.MA(close, timeperiod=5)[-1]
            features['ma10'] = talib.MA(close, timeperiod=10)[-1]
            features['ma20'] = talib.MA(close, timeperiod=20)[-1]
            
            # MACD
            macd, signal, hist = talib.MACD(close)
            features['macd'] = macd[-1] if len(macd) > 0 else 0
            features['macd_signal'] = signal[-1] if len(signal) > 0 else 0
            features['macd_hist'] = hist[-1] if len(hist) > 0 else 0
            
            # RSI
            features['rsi_6'] = talib.RSI(close, timeperiod=6)[-1]
            features['rsi_12'] = talib.RSI(close, timeperiod=12)[-1]
            
            # KDJ
            k, d = talib.STOCH(high, low, close)
            features['kdj_k'] = k[-1] if len(k) > 0 else 0
            features['kdj_d'] = d[-1] if len(d) > 0 else 0
            features['kdj_j'] = 3 * features['kdj_k'] - 2 * features['kdj_d']
            
            # æ›´å¤šæŠ€æœ¯æŒ‡æ ‡...
            
        except Exception as e:
            logging.error(f"æŠ€æœ¯æŒ‡æ ‡è®¡ç®—å¤±è´¥: {str(e)}")
            for key in features:
                if pd.isna(features[key]):
                    features[key] = 0
        
        return features
    
    def extract_all_features(self, stock_code, end_date):
        """æå–æ‰€æœ‰134ä¸ªåŸå§‹ç‰¹å¾"""
        try:
            df = self.load_data(stock_code, end_date)
            
            features = {'stock_code': stock_code, 'date': end_date}
            
            features.update(self.extract_price_features(df))
            features.update(self.extract_volume_features(df))
            features.update(self.extract_technical_indicators(df))
            
            logging.info(f"ç‰¹å¾æå–å®Œæˆ: {stock_code}, å…±{len(features)}ä¸ªç‰¹å¾")
            return features
        
        except Exception as e:
            logging.error(f"ç‰¹å¾æå–å¤±è´¥ {stock_code}: {str(e)}")
            return None

if __name__ == '__main__':
    extractor = FeatureExtractor(time_window=20)
    # extractor.extract_all_features('000001', datetime.now())
'''

    ALL_FILE_TEMPLATES["scripts/feature_engineering/ai_feature_synthesis.py"] = '''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIç‰¹å¾åˆæˆæ¨¡å— - åˆæˆ10ä¸ªAIç‰¹å¾
æƒé‡: æœ€é«˜ä¼˜å…ˆçº§ï¼ˆ1.5å€ï¼‰
"""

import pandas as pd
import numpy as np
from pathlib import Path
import logging

logging.basicConfig(
    filename='logs/ai_feature_synthesis.log',
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

class AIFeatureSynthesizer:
    """AIç‰¹å¾åˆæˆå™¨ - åˆæˆ10ä¸ªé«˜çº§ç‰¹å¾"""
    
    def __init__(self):
        self.feature_dir = Path('data/features')
        self.feature_dir.mkdir(parents=True, exist_ok=True)
    
    def synthesize_capital_flow_score(self, df_raw):
        """åˆæˆèµ„é‡‘æµå‘è¯„åˆ†"""
        score = (
            df_raw['volume_mean'] * 0.3 +
            df_raw['volume_change_mean'] * 0.4 +
            df_raw['volume_price_corr'] * 0.3
        )
        return score
    
    def synthesize_technical_pattern_score(self, df_raw):
        """åˆæˆæŠ€æœ¯å½¢æ€è¯„åˆ†"""
        score = (
            df_raw['macd_hist'] * 0.3 +
            df_raw['rsi_12'] / 100 * 0.3 +
            df_raw['kdj_j'] / 100 * 0.4
        )
        return score
    
    def synthesize_market_sentiment_score(self, df_raw):
        """åˆæˆå¸‚åœºæƒ…ç»ªè¯„åˆ†"""
        score = (
            df_raw['positive_days'] / 20 * 0.5 +
            df_raw['cumulative_return'] * 0.5
        )
        return score
    
    # ... å…¶ä»–7ä¸ªAIç‰¹å¾åˆæˆæ–¹æ³•
    
    def synthesize_all_ai_features(self, df_raw):
        """åˆæˆæ‰€æœ‰10ä¸ªAIç‰¹å¾"""
        df_ai = pd.DataFrame()
        
        df_ai['stock_code'] = df_raw['stock_code']
        df_ai['date'] = df_raw['date']
        
        # 10ä¸ªAIåˆæˆç‰¹å¾
        df_ai['ai_capital_flow'] = self.synthesize_capital_flow_score(df_raw)
        df_ai['ai_technical_pattern'] = self.synthesize_technical_pattern_score(df_raw)
        df_ai['ai_market_sentiment'] = self.synthesize_market_sentiment_score(df_raw)
        # ... å…¶ä»–7ä¸ªç‰¹å¾
        
        # æ ‡å‡†åŒ–åˆ°[0, 1]èŒƒå›´
        for col in df_ai.columns[2:]:
            df_ai[col] = (df_ai[col] - df_ai[col].min()) / (df_ai[col].max() - df_ai[col].min() + 1e-10)
        
        # ä¿å­˜
        output_file = self.feature_dir / 'ai_synthetic_features_10.csv'
        df_ai.to_csv(output_file, index=False)
        
        logging.info(f"AIç‰¹å¾åˆæˆå®Œæˆ: {len(df_ai)}æ¡è®°å½•")
        return df_ai

if __name__ == '__main__':
    synthesizer = AIFeatureSynthesizer()
'''

    ALL_FILE_TEMPLATES["scripts/feature_engineering/chip_distribution.py"] = '''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç­¹ç åˆ†å¸ƒæ¨¡å— - è®¡ç®—10ä¸ªç­¹ç ç‰¹å¾
æƒé‡: 0.2ï¼ˆåœ¨ç›¸ä¼¼åº¦ç­›é€‰ä¸­ï¼‰
"""

import pandas as pd
import numpy as np
from pathlib import Path
import logging

logging.basicConfig(
    filename='logs/chip_distribution.log',
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

class ChipDistributionAnalyzer:
    """ç­¹ç åˆ†å¸ƒåˆ†æå™¨ - è®¡ç®—10ä¸ªç­¹ç ç‰¹å¾"""
    
    def __init__(self):
        self.feature_dir = Path('data/features')
        self.feature_dir.mkdir(parents=True, exist_ok=True)
    
    def calculate_chip_concentration(self, df_price):
        """è®¡ç®—ç­¹ç é›†ä¸­åº¦"""
        df_price['turnover'] = df_price['close'] * df_price['volume']
        total_turnover = df_price['turnover'].sum()
        
        chip_dist = df_price.groupby('close')['turnover'].sum() / total_turnover
        top_20_chips = chip_dist.nlargest(int(len(chip_dist) * 0.2)).sum()
        
        return top_20_chips
    
    def calculate_chip_lock_ratio(self, df_price):
        """è®¡ç®—ç­¹ç é”å®šç‡"""
        avg_volume = df_price['volume'].mean()
        low_volume_days = df_price[df_price['volume'] < avg_volume * 0.5]
        lock_ratio = len(low_volume_days) / len(df_price)
        
        return lock_ratio
    
    # ... å…¶ä»–8ä¸ªç­¹ç ç‰¹å¾è®¡ç®—æ–¹æ³•
    
    def extract_all_chip_features(self, stock_code, df_price):
        """æå–æ‰€æœ‰10ä¸ªç­¹ç ç‰¹å¾"""
        features = {
            'stock_code': stock_code,
            'chip_concentration': self.calculate_chip_concentration(df_price),
            'chip_lock_ratio': self.calculate_chip_lock_ratio(df_price),
            # ... å…¶ä»–8ä¸ªç­¹ç ç‰¹å¾
        }
        
        return features

if __name__ == '__main__':
    analyzer = ChipDistributionAnalyzer()
'''

    ALL_FILE_TEMPLATES["scripts/feature_engineering/feature_normalization.py"] = '''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç‰¹å¾æ ‡å‡†åŒ–æ¨¡å— - æ ‡å‡†åŒ–æ‰€æœ‰ç‰¹å¾å¹¶é€‰æ‹©æœ€ç»ˆ88ä¸ªç‰¹å¾
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
import joblib
from pathlib import Path
import logging

logging.basicConfig(
    filename='logs/feature_normalization.log',
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

class FeatureNormalizer:
    """ç‰¹å¾æ ‡å‡†åŒ–å™¨ - 154ä¸ªç‰¹å¾æ ‡å‡†åŒ–å¹¶é€‰æ‹©88ä¸ª"""
    
    def __init__(self):
        self.feature_dir = Path('data/features')
        self.model_dir = Path('models')
        self.model_dir.mkdir(parents=True, exist_ok=True)
        self.scaler = StandardScaler()
    
    def load_all_features(self):
        """åŠ è½½æ‰€æœ‰åŸå§‹ç‰¹å¾"""
        # åŠ è½½134ä¸ªåŸå§‹ç‰¹å¾
        df_raw = pd.read_csv(self.feature_dir / 'raw_features_134.csv')
        
        # åŠ è½½10ä¸ªAIç‰¹å¾
        df_ai = pd.read_csv(self.feature_dir / 'ai_synthetic_features_10.csv')
        
        # åŠ è½½10ä¸ªç­¹ç ç‰¹å¾
        df_chip = pd.read_csv(self.feature_dir / 'chip_features_10.csv')
        
        # åˆå¹¶ (134 + 10 + 10 = 154ä¸ªç‰¹å¾)
        df_all = df_raw.merge(df_ai, on=['stock_code', 'date'])
        df_all = df_all.merge(df_chip, on='stock_code')
        
        logging.info(f"åŠ è½½ç‰¹å¾å®Œæˆ: {df_all.shape}")
        return df_all
    
    def normalize_features(self, df):
        """æ ‡å‡†åŒ–ç‰¹å¾ï¼ˆå‡å€¼0ï¼Œæ ‡å‡†å·®1ï¼‰"""
        id_columns = ['stock_code', 'date']
        feature_columns = [col for col in df.columns if col not in id_columns]
        
        df_normalized = df.copy()
        df_normalized[feature_columns] = self.scaler.fit_transform(df[feature_columns])
        
        # ä¿å­˜scaler
        scaler_file = self.model_dir / 'feature_scaler.pkl'
        joblib.dump(self.scaler, scaler_file)
        logging.info(f"Scalerå·²ä¿å­˜: {scaler_file}")
        
        return df_normalized
    
    def select_top_features(self, df, top_n=88):
        """æ ¹æ®ç‰¹å¾é‡è¦æ€§é€‰æ‹©Top 88ç‰¹å¾"""
        # ä¼˜å…ˆAIç‰¹å¾å’Œç­¹ç ç‰¹å¾
        ai_features = [col for col in df.columns if col.startswith('ai_')]
        chip_features = [col for col in df.columns if col.startswith('chip_')]
        
        other_features = [col for col in df.columns 
                        if col not in ['stock_code', 'date'] 
                        and not col.startswith('ai_') 
                        and not col.startswith('chip_')]
        
        # ç»„åˆ: 10ä¸ªAI + 10ä¸ªç­¹ç  + 68ä¸ªå…¶ä»–
        top_features = ai_features + chip_features + other_features[:68]
        
        selected_columns = ['stock_code', 'date'] + top_features[:88]
        df_selected = df[selected_columns]
        
        logging.info(f"ç‰¹å¾é€‰æ‹©å®Œæˆ: {len(top_features)}ä¸ªç‰¹å¾")
        return df_selected
    
    def process_all(self):
        """å®Œæ•´å¤„ç†æµç¨‹"""
        df_all = self.load_all_features()
        df_normalized = self.normalize_features(df_all)
        
        output_file_all = self.feature_dir / 'normalized_features_all.csv'
        df_normalized.to_csv(output_file_all, index=False)
        
        df_final = self.select_top_features(df_normalized, top_n=88)
        
        output_file_final = self.feature_dir / 'all_features_88.csv'
        df_final.to_csv(output_file_final, index=False)
        
        logging.info(f"æœ€ç»ˆ88ç‰¹å¾å·²ä¿å­˜: {output_file_final}")
        return df_final

if __name__ == '__main__':
    normalizer = FeatureNormalizer()
    df_final = normalizer.process_all()
'''

    ALL_FILE_TEMPLATES["scripts/feature_engineering/feature_importance.py"] = '''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç‰¹å¾é‡è¦æ€§åˆ†ææ¨¡å— - è®¡ç®—å¹¶æ’åºç‰¹å¾é‡è¦æ€§
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import mutual_info_classif
from pathlib import Path
import logging

logging.basicConfig(
    filename='logs/feature_importance.log',
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

class FeatureImportanceAnalyzer:
    """ç‰¹å¾é‡è¦æ€§åˆ†æå™¨"""
    
    def __init__(self):
        self.feature_dir = Path('data/features')
        self.output_dir = Path('data/features')
    
    def calculate_rf_importance(self, X, y):
        """åŸºäºéšæœºæ£®æ—è®¡ç®—ç‰¹å¾é‡è¦æ€§"""
        rf = RandomForestClassifier(n_estimators=100, random_state=42)
        rf.fit(X, y)
        
        importances = rf.feature_importances_
        return importances
    
    def analyze_feature_importance(self):
        """åˆ†æç‰¹å¾é‡è¦æ€§"""
        logging.info("ç‰¹å¾é‡è¦æ€§åˆ†ææ¨¡å—å·²å‡†å¤‡")
        # TODO: å®ç°å®Œæ•´çš„ç‰¹å¾é‡è¦æ€§åˆ†æ
        return None

if __name__ == '__main__':
    analyzer = FeatureImportanceAnalyzer()
'''

    #####################################################################
    # å¯¹ç…§ç»„æ¨¡å—
    #####################################################################
    
    ALL_FILE_TEMPLATES["scripts/contrast_group/identify_contrast_group.py"] = '''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¯†åˆ«å¯¹ç…§ç»„ - ç­›é€‰30ä¸ªäº¤æ˜“æ—¥è·Œå¹…å‰20åçš„è‚¡ç¥¨
"""

import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime, timedelta
import logging

logging.basicConfig(
    filename='logs/contrast_group.log',
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

class ContrastGroupIdentifier:
    """å¯¹ç…§ç»„è¯†åˆ«å™¨ - è¯†åˆ«è·Œå¹…å‰20å"""
    
    def __init__(self, period_days=30, top_n=20):
        self.period_days = period_days  # 30ä¸ªäº¤æ˜“æ—¥
        self.top_n = top_n  # å‰20å
        self.data_dir = Path('data/processed')
        self.output_dir = Path('data/contrast_group')
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def load_market_data(self):
        """åŠ è½½å¸‚åœºæ•°æ®"""
        df = pd.read_csv(self.data_dir / 'daily_data_cleaned.csv')
        return df
    
    def calculate_30d_return(self, df):
        """è®¡ç®—30ä¸ªäº¤æ˜“æ—¥æ¶¨è·Œå¹…"""
        df = df.sort_values(['stock_code', 'date'])
        
        results = []
        for stock_code, group in df.groupby('stock_code'):
            if len(group) >= self.period_days:
                start_price = group.iloc[0]['close']
                end_price = group.iloc[-1]['close']
                return_30d = (end_price - start_price) / start_price
                
                results.append({
                    'stock_code': stock_code,
                    'start_date': group.iloc[0]['date'],
                    'end_date': group.iloc[-1]['date'],
                    'start_price': start_price,
                    'end_price': end_price,
                    'return_30d': return_30d
                })
        
        return pd.DataFrame(results)
    
    def identify_worst_performers(self, df_returns):
        """è¯†åˆ«è·Œå¹…å‰20å"""
        df_sorted = df_returns.sort_values('return_30d')
        contrast_group = df_sorted.head(self.top_n)
        
        logging.info(f"è¯†åˆ«å¯¹ç…§ç»„å®Œæˆ: {len(contrast_group)}åªè‚¡ç¥¨")
        return contrast_group
    
    def save_contrast_group(self, contrast_group):
        """ä¿å­˜å¯¹ç…§ç»„"""
        output_file = self.output_dir / f'contrast_group_30d_drop_top20_{datetime.now().strftime("%Y%m%d")}.csv'
        contrast_group.to_csv(output_file, index=False)
        logging.info(f"å¯¹ç…§ç»„å·²ä¿å­˜: {output_file}")
        return output_file
    
    def run(self):
        """æ‰§è¡Œè¯†åˆ«æµç¨‹"""
        df = self.load_market_data()
        df_returns = self.calculate_30d_return(df)
        contrast_group = self.identify_worst_performers(df_returns)
        output_file = self.save_contrast_group(contrast_group)
        
        print(f"âœ… å¯¹ç…§ç»„è¯†åˆ«å®Œæˆ: {len(contrast_group)}åªè‚¡ç¥¨")
        return contrast_group

if __name__ == '__main__':
    identifier = ContrastGroupIdentifier(period_days=30, top_n=20)
    contrast_group = identifier.run()
'''

    ALL_FILE_TEMPLATES["scripts/contrast_group/extract_contrast_features.py"] = '''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æå–å¯¹ç…§ç»„ç‰¹å¾ - ä¸ºå¯¹ç…§ç»„ç”Ÿæˆç›¸åŒçš„88ä¸ªç‰¹å¾
"""

import pandas as pd
import sys
from pathlib import Path
import logging

logging.basicConfig(
    filename='logs/contrast_group.log',
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

class ContrastFeatureExtractor:
    """å¯¹ç…§ç»„ç‰¹å¾æå–å™¨"""
    
    def __init__(self):
        self.contrast_dir = Path('data/contrast_group')
        self.feature_dir = Path('data/features')
    
    def load_contrast_group(self):
        """åŠ è½½å¯¹ç…§ç»„è‚¡ç¥¨åˆ—è¡¨"""
        files = list(self.contrast_dir.glob('contrast_group_30d_drop_top20_*.csv'))
        if not files:
            raise FileNotFoundError("æœªæ‰¾åˆ°å¯¹ç…§ç»„æ–‡ä»¶")
        
        latest_file = max(files, key=lambda x: x.stat().st_mtime)
        df = pd.read_csv(latest_file)
        
        logging.info(f"åŠ è½½å¯¹ç…§ç»„: {len(df)}åªè‚¡ç¥¨")
        return df['stock_code'].tolist()
    
    def extract_features_for_contrast_group(self):
        """ä¸ºå¯¹ç…§ç»„æå–88ä¸ªç‰¹å¾"""
        stock_list = self.load_contrast_group()
        
        # TODO: è°ƒç”¨ç‰¹å¾æå–æ¨¡å—ä¸ºå¯¹ç…§ç»„æå–ç‰¹å¾
        logging.info(f"å¯¹ç…§ç»„ç‰¹å¾æå–å¼€å§‹: {len(stock_list)}åªè‚¡ç¥¨")
        
        return None
    
    def run(self):
        """æ‰§è¡Œæå–æµç¨‹"""
        df_features = self.extract_features_for_contrast_group()
        print(f"âœ… å¯¹ç…§ç»„ç‰¹å¾æå–å®Œæˆ")
        return df_features

if __name__ == '__main__':
    extractor = ContrastFeatureExtractor()
    df_features = extractor.run()
'''

    ALL_FILE_TEMPLATES["scripts/contrast_group/compare_contrast_vs_candidates.py"] = '''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¯¹æ¯”åˆ†æ - å¯¹ç…§ç»„ç‰¹å¾ vs å€™é€‰è‚¡ç¥¨ç‰¹å¾å¯¹æ¯”
è¾“å‡º: åˆ†ç¦»åº¦è¯„åˆ†ï¼ˆSeparation Scoreï¼‰
"""

import pandas as pd
import numpy as np
from scipy.spatial.distance import euclidean, cosine
import json
from pathlib import Path
from datetime import datetime
import logging

logging.basicConfig(
    filename='logs/contrast_group.log',
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

class ContrastComparator:
    """å¯¹ç…§ç»„å¯¹æ¯”åˆ†æå™¨"""
    
    def __init__(self):
        self.contrast_dir = Path('data/contrast_group')
        self.feature_dir = Path('data/features')
        self.output_dir = Path('data/analysis')
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def load_features(self):
        """åŠ è½½å¯¹ç…§ç»„å’Œå€™é€‰è‚¡ç¥¨ç‰¹å¾"""
        df_contrast = pd.read_csv(self.contrast_dir / 'contrast_group_features_88.csv')
        df_candidates = pd.read_csv(self.feature_dir / 'all_features_88.csv')
        
        return df_contrast, df_candidates
    
    def calculate_feature_separation(self, df_contrast, df_candidates):
        """è®¡ç®—ç‰¹å¾åˆ†ç¦»åº¦"""
        feature_cols = [col for col in df_contrast.columns if col not in ['stock_code', 'date']]
        
        contrast_mean = df_contrast[feature_cols].mean().values
        candidates_mean = df_candidates[feature_cols].mean().values
        
        euclidean_dist = euclidean(contrast_mean, candidates_mean)
        cosine_dist = cosine(contrast_mean, candidates_mean)
        
        return {
            'euclidean_distance': float(euclidean_dist),
            'cosine_distance': float(cosine_dist)
        }
    
    def calculate_separation_score(self, distances):
        """è®¡ç®—ç»¼åˆåˆ†ç¦»åº¦è¯„åˆ†ï¼ˆ0-1ï¼Œè¶Šé«˜è¶Šå¥½ï¼‰"""
        euclidean_score = min(distances['euclidean_distance'] / 10, 1.0)
        cosine_score = distances['cosine_distance']
        
        separation_score = euclidean_score * 0.5 + cosine_score * 0.5
        
        return float(separation_score)
    
    def run_comparison(self):
        """æ‰§è¡Œå®Œæ•´å¯¹æ¯”åˆ†æ"""
        df_contrast, df_candidates = self.load_features()
        
        distances = self.calculate_feature_separation(df_contrast, df_candidates)
        separation_score = self.calculate_separation_score(distances)
        
        results = {
            'comparison_date': datetime.now().strftime('%Y-%m-%d'),
            'contrast_group_size': len(df_contrast),
            'candidates_size': len(df_candidates),
            'distances': distances,
            'separation_score': separation_score
        }
        
        output_file = self.output_dir / 'contrast_vs_candidates_comparison.json'
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        logging.info(f"å¯¹æ¯”åˆ†æå®Œæˆ: {output_file}")
        logging.info(f"åˆ†ç¦»åº¦è¯„åˆ†: {separation_score:.4f}")
        
        print(f"\\nâœ… å¯¹æ¯”åˆ†æå®Œæˆ:")
        print(f"  ğŸ“Š åˆ†ç¦»åº¦è¯„åˆ†: {separation_score:.4f}")
        
        return results

if __name__ == '__main__':
    comparator = ContrastComparator()
    results = comparator.run_comparison()
'''

    #####################################################################
    # MLè®­ç»ƒæ¨¡å—
    #####################################################################
    
    ALL_FILE_TEMPLATES["scripts/ml_training/model_training.py"] = '''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨¡å‹è®­ç»ƒæ¨¡å— - è®­ç»ƒ3ä¸ªåŸºç¡€æ¨¡å‹
æ¨¡å‹: Logistic Regression, Random Forest, XGBoost/LightGBM
"""

import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb
from sklearn.model_selection import train_test_split
import joblib
from pathlib import Path
import logging

logging.basicConfig(
    filename='logs/ml_training.log',
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

class ModelTrainer:
    """æ¨¡å‹è®­ç»ƒå™¨ - è®­ç»ƒLRã€RFã€XGBä¸‰ä¸ªæ¨¡å‹"""
    
    def __init__(self):
        self.feature_dir = Path('data/features')
        self.contrast_dir = Path('data/contrast_group')
        self.model_dir = Path('models')
        self.model_dir.mkdir(parents=True, exist_ok=True)
    
    def load_training_data(self):
        """åŠ è½½è®­ç»ƒæ•°æ®ï¼ˆæˆåŠŸæ ·æœ¬ + å¯¹ç…§ç»„ï¼‰"""
        df_success = pd.read_csv(self.feature_dir / 'all_features_88.csv')
        df_success['label'] = 1
        
        df_contrast = pd.read_csv(self.contrast_dir / 'contrast_group_features_88.csv')
        df_contrast['label'] = 0
        
        df_train = pd.concat([df_success, df_contrast], ignore_index=True)
        
        logging.info(f"è®­ç»ƒæ•°æ®åŠ è½½å®Œæˆ: {len(df_train)}æ¡")
        return df_train
    
    def prepare_data(self, df):
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        X = df.drop(['stock_code', 'date', 'label'], axis=1)
        y = df['label']
        
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        return X_train, X_test, y_train, y_test
    
    def train_logistic_regression(self, X_train, y_train):
        """è®­ç»ƒé€»è¾‘å›å½’æ¨¡å‹"""
        logging.info("å¼€å§‹è®­ç»ƒé€»è¾‘å›å½’æ¨¡å‹...")
        
        model = LogisticRegression(
            max_iter=1000,
            random_state=42,
            class_weight='balanced'
        )
        model.fit(X_train, y_train)
        
        model_file = self.model_dir / 'model_lr.pkl'
        joblib.dump(model, model_file)
        
        logging.info(f"é€»è¾‘å›å½’æ¨¡å‹å·²ä¿å­˜: {model_file}")
        return model
    
    def train_random_forest(self, X_train, y_train):
        """è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹"""
        logging.info("å¼€å§‹è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹...")
        
        model = RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            random_state=42,
            class_weight='balanced',
            n_jobs=-1
        )
        model.fit(X_train, y_train)
        
        model_file = self.model_dir / 'model_rf.pkl'
        joblib.dump(model, model_file)
        
        logging.info(f"éšæœºæ£®æ—æ¨¡å‹å·²ä¿å­˜: {model_file}")
        return model
    
    def train_gradient_boosting(self, X_train, y_train):
        """è®­ç»ƒæ¢¯åº¦æå‡æ¨¡å‹"""
        logging.info(f"å¼€å§‹è®­ç»ƒæ¢¯åº¦æå‡æ¨¡å‹(XGBoost)...")
        
        model = xgb.XGBClassifier(
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            random_state=42,
            use_label_encoder=False,
            eval_metric='logloss'
        )
        
        model.fit(X_train, y_train)
        
        model_file = self.model_dir / f'model_gb_xgboost.pkl'
        joblib.dump(model, model_file)
        
        logging.info(f"æ¢¯åº¦æå‡æ¨¡å‹å·²ä¿å­˜: {model_file}")
        return model
    
    def train_all_models(self):
        """è®­ç»ƒæ‰€æœ‰3ä¸ªæ¨¡å‹"""
        df = self.load_training_data()
        X_train, X_test, y_train, y_test = self.prepare_data(df)
        
        model_lr = self.train_logistic_regression(X_train, y_train)
        model_rf = self.train_random_forest(X_train, y_train)
        model_gb = self.train_gradient_boosting(X_train, y_train)
        
        test_data = {
            'X_test': X_test,
            'y_test': y_test
        }
        joblib.dump(test_data, self.model_dir / 'test_data.pkl')
        
        logging.info("æ‰€æœ‰æ¨¡å‹è®­ç»ƒå®Œæˆ")
        
        print("\\nâœ… æ¨¡å‹è®­ç»ƒå®Œæˆ:")
        print("  ğŸ“Š é€»è¾‘å›å½’: model_lr.pkl")
        print("  ğŸŒ² éšæœºæ£®æ—: model_rf.pkl")
        print("  ğŸš€ æ¢¯åº¦æå‡: model_gb_xgboost.pkl")
        
        return {
            'lr': model_lr,
            'rf': model_rf,
            'gb': model_gb
        }

if __name__ == '__main__':
    trainer = ModelTrainer()
    models = trainer.train_all_models()
'''

    ALL_FILE_TEMPLATES["scripts/ml_training/model_ensemble.py"] = '''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨¡å‹é›†æˆ - é›†æˆ3ä¸ªåŸºç¡€æ¨¡å‹
æ–¹æ³•: åŠ æƒå¹³å‡ (LR: 0.4, RF: 0.3, GB: 0.3)
"""

import pandas as pd
import numpy as np
import joblib
import json
from pathlib import Path
import logging

logging.basicConfig(
    filename='logs/ml_training.log',
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

class ModelEnsembler:
    """æ¨¡å‹é›†æˆå™¨"""
    
    def __init__(self, weights=None):
        self.model_dir = Path('models')
        self.weights = weights or {'lr': 0.4, 'rf': 0.3, 'gb': 0.3}
    
    def load_models(self):
        """åŠ è½½3ä¸ªåŸºç¡€æ¨¡å‹"""
        model_lr = joblib.load(self.model_dir / 'model_lr.pkl')
        model_rf = joblib.load(self.model_dir / 'model_rf.pkl')
        model_gb = joblib.load(self.model_dir / 'model_gb_xgboost.pkl')
        
        return {
            'lr': model_lr,
            'rf': model_rf,
            'gb': model_gb
        }
    
    def save_ensemble_weights(self):
        """ä¿å­˜é›†æˆæƒé‡"""
        weights_file = self.model_dir / 'ensemble_weights.json'
        with open(weights_file, 'w') as f:
            json.dump(self.weights, f, indent=2)
        
        logging.info(f"é›†æˆæƒé‡å·²ä¿å­˜: {weights_file}")
    
    def create_ensemble(self):
        """åˆ›å»ºé›†æˆæ¨¡å‹"""
        models = self.load_models()
        
        self.save_ensemble_weights()
        
        class EnsembleModel:
            def __init__(self, models, weights):
                self.models = models
                self.weights = weights
            
            def predict_proba(self, X):
                proba_lr = self.models['lr'].predict_proba(X)[:, 1]
                proba_rf = self.models['rf'].predict_proba(X)[:, 1]
                proba_gb = self.models['gb'].predict_proba(X)[:, 1]
                
                proba = (
                    proba_lr * self.weights['lr'] +
                    proba_rf * self.weights['rf'] +
                    proba_gb * self.weights['gb']
                )
                return proba
            
            def predict(self, X, threshold=0.5):
                proba = self.predict_proba(X)
                return (proba >= threshold).astype(int)
        
        ensemble_model = EnsembleModel(models, self.weights)
        
        model_file = self.model_dir / 'model_ensemble.pkl'
        joblib.dump(ensemble_model, model_file)
        
        logging.info(f"é›†æˆæ¨¡å‹å·²ä¿å­˜: {model_file}")
        
        print("\\nâœ… æ¨¡å‹é›†æˆå®Œæˆ:")
        print(f"  æƒé‡é…ç½®: {self.weights}")
        print(f"  é›†æˆæ¨¡å‹: {model_file}")
        
        return ensemble_model

if __name__ == '__main__':
    ensembler = ModelEnsembler()
    ensemble_model = ensembler.create_ensemble()
'''

    ALL_FILE_TEMPLATES["scripts/ml_training/model_evaluation.py"] = '''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨¡å‹è¯„ä¼° - è¯„ä¼°æ¨¡å‹æ€§èƒ½
æŒ‡æ ‡: Accuracy, Precision, Recall, F1, AUC-ROC
"""

import pandas as pd
import numpy as np
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix
)
import joblib
import json
from pathlib import Path
import logging

logging.basicConfig(
    filename='logs/ml_training.log',
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

class ModelEvaluator:
    """æ¨¡å‹è¯„ä¼°å™¨"""
    
    def __init__(self):
        self.model_dir = Path('models')
        self.output_dir = Path('reports/model_evaluation')
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def load_test_data(self):
        """åŠ è½½æµ‹è¯•æ•°æ®"""
        test_data = joblib.load(self.model_dir / 'test_data.pkl')
        return test_data['X_test'], test_data['y_test']
    
    def evaluate_model(self, model, X_test, y_test, model_name):
        """è¯„ä¼°å•ä¸ªæ¨¡å‹"""
        y_pred = model.predict(X_test)
        
        if hasattr(model, 'predict_proba'):
            y_proba = model.predict_proba(X_test)[:, 1]
        else:
            y_proba = y_pred
        
        metrics = {
            'model_name': model_name,
            'accuracy': float(accuracy_score(y_test, y_pred)),
            'precision': float(precision_score(y_test, y_pred)),
            'recall': float(recall_score(y_test, y_pred)),
            'f1_score': float(f1_score(y_test, y_pred)),
            'auc_roc': float(roc_auc_score(y_test, y_proba))
        }
        
        cm = confusion_matrix(y_test, y_pred)
        metrics['confusion_matrix'] = cm.tolist()
        
        return metrics
    
    def evaluate_all_models(self):
        """è¯„ä¼°æ‰€æœ‰æ¨¡å‹"""
        X_test, y_test = self.load_test_data()
        
        model_lr = joblib.load(self.model_dir / 'model_lr.pkl')
        model_rf = joblib.load(self.model_dir / 'model_rf.pkl')
        model_gb = joblib.load(self.model_dir / 'model_gb_xgboost.pkl')
        
        metrics_lr = self.evaluate_model(model_lr, X_test, y_test, 'Logistic Regression')
        metrics_rf = self.evaluate_model(model_rf, X_test, y_test, 'Random Forest')
        metrics_gb = self.evaluate_model(model_gb, X_test, y_test, 'Gradient Boosting')
        
        model_ensemble = joblib.load(self.model_dir / 'model_ensemble.pkl')
        metrics_ensemble = self.evaluate_model(model_ensemble, X_test, y_test, 'Ensemble')
        
        all_metrics = {
            'logistic_regression': metrics_lr,
            'random_forest': metrics_rf,
            'gradient_boosting': metrics_gb,
            'ensemble': metrics_ensemble
        }
        
        output_file = self.output_dir / 'model_metrics.json'
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(all_metrics, f, ensure_ascii=False, indent=2)
        
        logging.info(f"æ¨¡å‹è¯„ä¼°å®Œæˆ: {output_file}")
        
        self.generate_evaluation_report(all_metrics)
        
        return all_metrics
    
    def generate_evaluation_report(self, metrics):
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        report = []
        report.append("# æ¨¡å‹è¯„ä¼°æŠ¥å‘Š\\n")
        report.append("## è¯„ä¼°æŒ‡æ ‡\\n")
        
        report.append("| æ¨¡å‹ | Accuracy | Precision | Recall | F1 Score | AUC-ROC |")
        report.append("|------|----------|-----------|--------|----------|---------|")
        
        for model_name, m in metrics.items():
            report.append(
                f"| {m['model_name']} | "
                f"{m['accuracy']:.4f} | "
                f"{m['precision']:.4f} | "
                f"{m['recall']:.4f} | "
                f"{m['f1_score']:.4f} | "
                f"{m['auc_roc']:.4f} |"
            )
        
        report.append("\\n## ç»“è®º\\n")
        
        best_model = max(metrics.items(), key=lambda x: x[1]['f1_score'])
        report.append(f"æœ€ä½³æ¨¡å‹: **{best_model[1]['model_name']}** (F1 Score: {best_model[1]['f1_score']:.4f})\\n")
        
        report_file = self.output_dir / 'evaluation_report.md'
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write('\\n'.join(report))
        
        logging.info(f"è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
        print("\\nâœ… æ¨¡å‹è¯„ä¼°å®Œæˆ:")
        for model_name, m in metrics.items():
            print(f"  {m['model_name']}: Accuracy={m['accuracy']:.4f}, F1={m['f1_score']:.4f}")

if __name__ == '__main__':
    evaluator = ModelEvaluator()
    metrics = evaluator.evaluate_all_models()
'''

    #####################################################################
    # ç­›é€‰æ¨¡å—
    #####################################################################
    
    ALL_FILE_TEMPLATES["scripts/filtering/similarity_filter.py"] = '''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç›¸ä¼¼åº¦ç­›é€‰ä¸»ç¨‹åº
æƒé‡: MLæ¨¡å‹(0.6) + ç­¹ç åˆ†å¸ƒ(0.2) + æ¶ˆæ¯é¢(0.2)
"""

import pandas as pd
import numpy as np
import joblib
import json
from pathlib import Path
import logging

logging.basicConfig(
    filename='logs/filtering.log',
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

class SimilarityFilter:
    """ç›¸ä¼¼åº¦ç­›é€‰å™¨"""
    
    def __init__(self):
        self.model_dir = Path('models')
        self.feature_dir = Path('data/features')
        self.config_dir = Path('config')
        self.output_dir = Path('data/candidates')
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.weights = self.load_weights()
    
    def load_weights(self):
        """åŠ è½½æƒé‡é…ç½®"""
        weights_file = self.config_dir / 'weights.json'
        if weights_file.exists():
            with open(weights_file, 'r') as f:
                return json.load(f)
        else:
            return {
                'ml_model': 0.6,
                'chip_distribution': 0.2,
                'news_sentiment': 0.2
            }
    
    def load_candidates(self):
        """åŠ è½½å€™é€‰è‚¡ç¥¨"""
        df = pd.read_csv(self.feature_dir / 'all_features_88.csv')
        return df
    
    def calculate_ml_similarity(self, df):
        """è®¡ç®—MLæ¨¡å‹ç›¸ä¼¼åº¦"""
        model = joblib.load(self.model_dir / 'model_ensemble.pkl')
        
        X = df.drop(['stock_code', 'date'], axis=1)
        similarity = model.predict_proba(X)
        
        return similarity
    
    def calculate_chip_similarity(self, df):
        """è®¡ç®—ç­¹ç åˆ†å¸ƒç›¸ä¼¼åº¦"""
        chip_features = [col for col in df.columns if col.startswith('chip_')]
        
        if len(chip_features) == 0:
            return np.ones(len(df))
        
        chip_scores = df[chip_features].mean(axis=1).values
        chip_scores = (chip_scores - chip_scores.min()) / (chip_scores.max() - chip_scores.min() + 1e-10)
        
        return chip_scores
    
    def calculate_news_similarity(self, df):
        """è®¡ç®—æ¶ˆæ¯é¢ç›¸ä¼¼åº¦ï¼ˆç”±æ¶ˆæ¯é¢åˆ†æå‘˜æä¾›ï¼‰"""
        # TODO: ä»æ¶ˆæ¯é¢åˆ†ææ¨¡å—è·å–è¯„åˆ†
        return np.ones(len(df)) * 0.5
    
    def calculate_combined_similarity(self, df):
        """è®¡ç®—ç»¼åˆç›¸ä¼¼åº¦"""
        ml_sim = self.calculate_ml_similarity(df)
        chip_sim = self.calculate_chip_similarity(df)
        news_sim = self.calculate_news_similarity(df)
        
        combined_sim = (
            ml_sim * self.weights['ml_model'] +
            chip_sim * self.weights['chip_distribution'] +
            news_sim * self.weights['news_sentiment']
        )
        
        return combined_sim, ml_sim, chip_sim, news_sim
    
    def filter_candidates(self, threshold=0.5):
        """æ ¹æ®é˜ˆå€¼ç­›é€‰å€™é€‰è‚¡ç¥¨"""
        df = self.load_candidates()
        
        combined_sim, ml_sim, chip_sim, news_sim = self.calculate_combined_similarity(df)
        
        df['ml_similarity'] = ml_sim
        df['chip_similarity'] = chip_sim
        df['news_similarity'] = news_sim
        df['combined_similarity'] = combined_sim
        
        df_filtered = df[df['combined_similarity'] >= threshold].copy()
        df_filtered = df_filtered.sort_values('combined_similarity', ascending=False)
        
        logging.info(f"ç­›é€‰å®Œæˆ: é˜ˆå€¼={threshold}, å€™é€‰æ•°={len(df_filtered)}")
        
        return df_filtered
    
    def filter_with_multiple_thresholds(self, thresholds=[0.5, 0.4, 0.3]):
        """ä½¿ç”¨å¤šä¸ªé˜ˆå€¼ç­›é€‰"""
        results = {}
        
        for threshold in thresholds:
            df_filtered = self.filter_candidates(threshold)
            
            output_file = self.output_dir / f'candidates_{int(threshold*100)}pct.csv'
            df_filtered.to_csv(output_file, index=False)
            
            results[f'{int(threshold*100)}%'] = {
                'threshold': threshold,
                'count': len(df_filtered),
                'file': str(output_file)
            }
            
            logging.info(f"é˜ˆå€¼{threshold}: {len(df_filtered)}åªå€™é€‰è‚¡ç¥¨")
        
        summary_file = self.output_dir / 'filtering_summary.json'
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print("\\nâœ… ç›¸ä¼¼åº¦ç­›é€‰å®Œæˆ:")
        for key, value in results.items():
            print(f"  {key} é˜ˆå€¼: {value['count']}åªè‚¡ç¥¨")
        
        return results

if __name__ == '__main__':
    filter = SimilarityFilter()
    results = filter.filter_with_multiple_thresholds([0.5, 0.4, 0.3])
'''

    ALL_FILE_TEMPLATES["scripts/filtering/filtering_logic.py"] = '''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é€’è¿›ç­›é€‰é€»è¾‘ - å…ˆ50% â†’ è‹¥æ— åˆ™40% â†’ è‹¥æ— åˆ™30%
å‚æ•°å¯é€šè¿‡Dashboardè°ƒæ•´
"""

import pandas as pd
import json
from pathlib import Path
import logging

logging.basicConfig(
    filename='logs/filtering.log',
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

class ProgressiveFilter:
    """é€’è¿›ç­›é€‰å™¨"""
    
    def __init__(self):
        self.candidates_dir = Path('data/candidates')
        self.config_dir = Path('config')
        self.output_dir = Path('data/candidates')
    
    def load_parameters(self):
        """åŠ è½½ç­›é€‰å‚æ•°"""
        params_file = self.config_dir / 'parameters.json'
        if params_file.exists():
            with open(params_file, 'r') as f:
                return json.load(f)
        else:
            return {
                'thresholds': [0.5, 0.4, 0.3],
                'min_candidates': 5,
                'max_candidates': 20
            }
    
    def progressive_filter(self):
        """æ‰§è¡Œé€’è¿›ç­›é€‰"""
        params = self.load_parameters()
        thresholds = params['thresholds']
        min_candidates = params['min_candidates']
        max_candidates = params['max_candidates']
        
        final_candidates = None
        selected_threshold = None
        
        for threshold in thresholds:
            file_name = f'candidates_{int(threshold*100)}pct.csv'
            file_path = self.candidates_dir / file_name
            
            if not file_path.exists():
                continue
            
            df = pd.read_csv(file_path)
            
            if len(df) >= min_candidates:
                if len(df) > max_candidates:
                    final_candidates = df.head(max_candidates)
                else:
                    final_candidates = df
                
                selected_threshold = threshold
                logging.info(f"é€‰æ‹©é˜ˆå€¼{threshold}: {len(final_candidates)}åªå€™é€‰è‚¡ç¥¨")
                break
        
        if final_candidates is None:
            logging.warning("æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„å€™é€‰è‚¡ç¥¨")
            return None
        
        output_file = self.output_dir / 'selection_candidates.csv'
        final_candidates.to_csv(output_file, index=False)
        
        selection_info = {
            'selected_threshold': selected_threshold,
            'candidates_count': len(final_candidates),
            'min_candidates': min_candidates,
            'max_candidates': max_candidates,
            'top_stocks': final_candidates['stock_code'].head(10).tolist()
        }
        
        info_file = self.output_dir / 'selection_info.json'
        with open(info_file, 'w', encoding='utf-8') as f:
            json.dump(selection_info, f, ensure_ascii=False, indent=2)
        
        logging.info(f"æœ€ç»ˆå€™é€‰å·²ä¿å­˜: {output_file}")
        
        print(f"\\nâœ… é€’è¿›ç­›é€‰å®Œæˆ:")
        print(f"  é€‰æ‹©é˜ˆå€¼: {selected_threshold}")
        print(f"  å€™é€‰æ•°é‡: {len(final_candidates)}")
        
        return final_candidates

if __name__ == '__main__':
    filter = ProgressiveFilter()
    candidates = filter.progressive_filter()
'''

    #####################################################################
    # è·Ÿè¸ªæŠ¥å‘Šæ¨¡å—
    #####################################################################
    
    ALL_FILE_TEMPLATES["scripts/tracking/track_candidates_30d.py"] = '''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
30å¤©è·Ÿè¸ªæ¨¡å— - è·Ÿè¸ªå€™é€‰è‚¡ç¥¨30ä¸ªäº¤æ˜“æ—¥çš„è¡¨ç°
"""

import pandas as pd
import json
from pathlib import Path
from datetime import datetime
import logging

logging.basicConfig(
    filename='logs/tracking.log',
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

class CandidateTracker:
    """å€™é€‰è‚¡ç¥¨è·Ÿè¸ªå™¨"""
    
    def __init__(self, tracking_days=30):
        self.tracking_days = tracking_days
        self.candidates_dir = Path('data/candidates')
        self.output_dir = Path('data/tracking')
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def load_candidates(self):
        """åŠ è½½å€™é€‰è‚¡ç¥¨"""
        df = pd.read_csv(self.candidates_dir / 'selection_candidates.csv')
        return df
    
    def track_performance(self):
        """è·Ÿè¸ª30å¤©è¡¨ç°"""
        candidates = self.load_candidates()
        
        # TODO: å®ç°30å¤©è·Ÿè¸ªé€»è¾‘
        tracking_data = {
            'tracking_start': datetime.now().strftime('%Y-%m-%d'),
            'tracking_days': self.tracking_days,
            'candidates': candidates['stock_code'].tolist()
        }
        
        output_file = self.output_dir / 'active_tracking.json'
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(tracking_data, f, ensure_ascii=False, indent=2)
        
        logging.info(f"è·Ÿè¸ªæ•°æ®å·²ä¿å­˜: {output_file}")
        return tracking_data

if __name__ == '__main__':
    tracker = CandidateTracker(tracking_days=30)
    tracker.track_performance()
'''

    ALL_FILE_TEMPLATES["scripts/tracking/performance_evaluation.py"] = '''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•ˆæœè¯„ä¼°æ¨¡å— - è¯„ä¼°å€™é€‰è‚¡ç¥¨è¡¨ç°
"""

import pandas as pd
from pathlib import Path
import logging

logging.basicConfig(
    filename='logs/tracking.log',
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

class PerformanceEvaluator:
    """è¡¨ç°è¯„ä¼°å™¨"""
    
    def __init__(self):
        self.tracking_dir = Path('data/tracking')
        self.output_dir = Path('reports')
    
    def evaluate_performance(self):
        """è¯„ä¼°è¡¨ç°"""
        # TODO: å®ç°æ•ˆæœè¯„ä¼°é€»è¾‘
        logging.info("æ•ˆæœè¯„ä¼°å®Œæˆ")
        return None

if __name__ == '__main__':
    evaluator = PerformanceEvaluator()
    evaluator.evaluate_performance()
'''

    ALL_FILE_TEMPLATES["scripts/tracking/generate_daily_report.py"] = '''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ—¥æŠ¥ç”Ÿæˆæ¨¡å—
"""

from datetime import datetime
from pathlib import Path

class DailyReportGenerator:
    """æ—¥æŠ¥ç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.output_dir = Path('reports/daily')
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def generate_report(self):
        """ç”Ÿæˆæ—¥æŠ¥"""
        today = datetime.now().strftime('%Y-%m-%d')
        report = f"# æ—¥æŠ¥ {today}\\n\\n## ä»Šæ—¥å€™é€‰è‚¡ç¥¨\\n\\nTODO: å®ç°æ—¥æŠ¥å†…å®¹\\n"
        
        output_file = self.output_dir / f"daily_{today}.md"
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"âœ… æ—¥æŠ¥å·²ç”Ÿæˆ: {output_file}")

if __name__ == '__main__':
    generator = DailyReportGenerator()
    generator.generate_report()
'''

    ALL_FILE_TEMPLATES["scripts/tracking/generate_weekly_report.py"] = '''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å‘¨æŠ¥ç”Ÿæˆæ¨¡å—
"""

from datetime import datetime
from pathlib import Path

class WeeklyReportGenerator:
    """å‘¨æŠ¥ç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.output_dir = Path('reports/weekly')
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def generate_report(self):
        """ç”Ÿæˆå‘¨æŠ¥"""
        week = datetime.now().isocalendar()[1]
        year = datetime.now().year
        report = f"# å‘¨æŠ¥ {year}-W{week:02d}\\n\\n## æœ¬å‘¨æ€»ç»“\\n\\nTODO: å®ç°å‘¨æŠ¥å†…å®¹\\n"
        
        output_file = self.output_dir / f"weekly_{year}-W{week:02d}.md"
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"âœ… å‘¨æŠ¥å·²ç”Ÿæˆ: {output_file}")

if __name__ == '__main__':
    generator = WeeklyReportGenerator()
    generator.generate_report()
'''

    ALL_FILE_TEMPLATES["scripts/tracking/generate_monthly_report.py"] = '''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœˆæŠ¥ç”Ÿæˆæ¨¡å—
"""

from datetime import datetime
from pathlib import Path

class MonthlyReportGenerator:
    """æœˆæŠ¥ç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.output_dir = Path('reports/monthly')
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def generate_report(self):
        """ç”ŸæˆæœˆæŠ¥"""
        month = datetime.now().strftime('%Y-%m')
        report = f"# æœˆæŠ¥ {month}\\n\\n## æœ¬æœˆæ€»ç»“\\n\\nTODO: å®ç°æœˆæŠ¥å†…å®¹\\n"
        
        output_file = self.output_dir / f"monthly_{month}.md"
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"âœ… æœˆæŠ¥å·²ç”Ÿæˆ: {output_file}")

if __name__ == '__main__':
    generator = MonthlyReportGenerator()
    generator.generate_report()
'''

    #####################################################################
    # GitHubè‡ªåŠ¨åŒ–æ¨¡å—
    #####################################################################
    
    ALL_FILE_TEMPLATES["scripts/github/github_trigger.py"] = '''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GitHubè§¦å‘æœºåˆ¶ - è§¦å‘GitHub Actionså·¥ä½œæµ
"""

import requests
import json
from pathlib import Path
import logging

logging.basicConfig(
    filename='logs/github.log',
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

class GitHubTrigger:
    """GitHubå·¥ä½œæµè§¦å‘å™¨"""
    
    def __init__(self):
        self.config_file = Path('config/config.json')
        self.load_config()
    
    def load_config(self):
        """åŠ è½½GitHubé…ç½®"""
        if self.config_file.exists():
            with open(self.config_file, 'r') as f:
                config = json.load(f)
                self.github_token = config.get('github_token', '')
                self.repo = config.get('github_repo', '')
    
    def trigger_workflow(self, workflow_name, inputs=None):
        """è§¦å‘å·¥ä½œæµ"""
        # TODO: å®ç°GitHub Actionsè§¦å‘é€»è¾‘
        logging.info(f"è§¦å‘å·¥ä½œæµ: {workflow_name}")
        print(f"âœ… å·¥ä½œæµå·²è§¦å‘: {workflow_name}")

if __name__ == '__main__':
    trigger = GitHubTrigger()
    trigger.trigger_workflow('daily.yml')
'''

    #####################################################################
    # ç›‘ç£æŠ¥å‘Šæ¨¡å—
    #####################################################################
    
    ALL_FILE_TEMPLATES["scripts/monitoring/generate_supervisory_report.py"] = '''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç›‘ç£æŠ¥å‘Šç”Ÿæˆæ¨¡å— - ä¸ºç›‘ç£å‘˜å·¥ç”ŸæˆæŠ¥å‘Š
"""

import json
from datetime import datetime
from pathlib import Path

class SupervisoryReportGenerator:
    """ç›‘ç£æŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.output_dir = Path('reports/supervisory')
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def generate_report(self):
        """ç”Ÿæˆç›‘ç£æŠ¥å‘Š"""
        report = {
            'report_date': datetime.now().strftime('%Y-%m-%d'),
            'project_status': 'running',
            'data_collection': 'normal',
            'model_performance': 'good',
            'issues': []
        }
        
        output_file = self.output_dir / 'supervisor_report.json'
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ç›‘ç£æŠ¥å‘Šå·²ç”Ÿæˆ: {output_file}")
        return report

if __name__ == '__main__':
    generator = SupervisoryReportGenerator()
    generator.generate_report()
'''

    #####################################################################
    # å·¥å…·ç±»æ¨¡å—
    #####################################################################
    
    ALL_FILE_TEMPLATES["scripts/utils/backup_manager.py"] = '''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤‡ä»½ç®¡ç†æ¨¡å—
"""

import shutil
from datetime import datetime, timedelta
from pathlib import Path
import logging

logging.basicConfig(
    filename='logs/backup.log',
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

class BackupManager:
    """å¤‡ä»½ç®¡ç†å™¨"""
    
    def __init__(self, retention_days=180):
        self.retention_days = retention_days
        self.backup_dir = Path('data/backup')
        self.backup_dir.mkdir(parents=True, exist_ok=True)
    
    def create_backup(self, source_dir, backup_name=None):
        """åˆ›å»ºå¤‡ä»½"""
        if backup_name is None:
            backup_name = f"backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        backup_path = self.backup_dir / backup_name
        shutil.copytree(source_dir, backup_path, dirs_exist_ok=True)
        
        logging.info(f"å¤‡ä»½å·²åˆ›å»º: {backup_path}")
        return backup_path
    
    def clean_old_backups(self):
        """æ¸…ç†è¿‡æœŸå¤‡ä»½"""
        cutoff_date = datetime.now() - timedelta(days=self.retention_days)
        
        for backup_dir in self.backup_dir.iterdir():
            if backup_dir.is_dir():
                # ä»ç›®å½•åæå–æ—¥æœŸ
                try:
                    date_str = backup_dir.name.split('_')[1]
                    backup_date = datetime.strptime(date_str, '%Y%m%d')
                    if backup_date < cutoff_date:
                        shutil.rmtree(backup_dir)
                        logging.info(f"åˆ é™¤è¿‡æœŸå¤‡ä»½: {backup_dir}")
                except:
                    pass

if __name__ == '__main__':
    manager = BackupManager(retention_days=180)
    manager.clean_old_backups()
'''

    ALL_FILE_TEMPLATES["scripts/utils/verify_backup.py"] = '''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤‡ä»½éªŒè¯æ¨¡å—
"""

from pathlib import Path
import hashlib
import logging

logging.basicConfig(
    filename='logs/backup.log',
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

class BackupVerifier:
    """å¤‡ä»½éªŒè¯å™¨"""
    
    def __init__(self):
        self.backup_dir = Path('data/backup')
    
    def calculate_checksum(self, file_path):
        """è®¡ç®—æ–‡ä»¶æ ¡éªŒå’Œ"""
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    
    def verify_backup(self, backup_name):
        """éªŒè¯å¤‡ä»½å®Œæ•´æ€§"""
        backup_path = self.backup_dir / backup_name
        if not backup_path.exists():
            logging.error(f"å¤‡ä»½ä¸å­˜åœ¨: {backup_name}")
            return False
        
        # TODO: å®ç°å¤‡ä»½éªŒè¯é€»è¾‘
        logging.info(f"å¤‡ä»½éªŒè¯å®Œæˆ: {backup_name}")
        return True

if __name__ == '__main__':
    verifier = BackupVerifier()
'''

    #####################################################################
    # é…ç½®æ–‡ä»¶
    #####################################################################
    
    ALL_FILE_TEMPLATES["config/config.json"] = '''{
  "project_name": "å±±è„šä¸‹é¡¹ç›® v2.0",
  "version": "2.0",
  "data_retention_days": 180,
  "feature_time_window": 20,
  "github_token": "YOUR_GITHUB_TOKEN",
  "github_repo": "laobai6630-dotcom/cto",
  "dashboard_password": "admin123",
  "api_endpoints": {
    "stock_data": "https://api.example.com/stock",
    "financial_data": "https://api.example.com/financial"
  }
}
'''

    ALL_FILE_TEMPLATES["config/weights.json"] = '''{
  "similarity_weights": {
    "ml_model": 0.6,
    "chip_distribution": 0.2,
    "news_sentiment": 0.2
  },
  "model_ensemble_weights": {
    "lr": 0.4,
    "rf": 0.3,
    "gb": 0.3
  }
}
'''

    ALL_FILE_TEMPLATES["config/parameters.json"] = '''{
  "filtering": {
    "thresholds": [0.5, 0.4, 0.3],
    "min_candidates": 5,
    "max_candidates": 20
  },
  "tracking": {
    "tracking_days": 30,
    "success_threshold": 0.5
  },
  "contrast_group": {
    "period_days": 30,
    "top_n": 20
  }
}
'''

    #####################################################################
    # æ–‡æ¡£æ–‡ä»¶
    #####################################################################
    
    ALL_FILE_TEMPLATES["docs/README.md"] = '''# å±±è„šä¸‹é¡¹ç›® v2.0

## é¡¹ç›®æ¦‚è¿°

Aè‚¡"å±±è„šä¸‹"å½¢æ€è‚¡ç¥¨ç²¾å‡†ç­›é€‰ç³»ç»Ÿ

### æ ¸å¿ƒåŠŸèƒ½

1. **æ•°æ®é‡‡é›†**: 180å¤©æ•°æ®ä¿ç•™æœŸ
2. **ç‰¹å¾å·¥ç¨‹**: 88ä¸ªç‰¹å¾ï¼ˆ134åŸå§‹+10AI+10ç­¹ç ï¼‰
3. **å¯¹ç…§ç»„åˆ†æ**: è·Œå¹…å‰20åè‚¡ç¥¨å¯¹æ¯”
4. **MLæ¨¡å‹**: 3ä¸ªæ¨¡å‹é›†æˆï¼ˆLR+RF+XGBï¼‰
5. **ç›¸ä¼¼åº¦ç­›é€‰**: é€’è¿›ç­›é€‰ï¼ˆ50%â†’40%â†’30%ï¼‰
6. **30å¤©è·Ÿè¸ª**: åŠ¨æ€è·Ÿè¸ªå€™é€‰è‚¡ç¥¨

### ç›®å½•ç»“æ„

```
shanjiaxia_project/
â”œâ”€â”€ scripts/          # æ‰€æœ‰Pythonè„šæœ¬
â”œâ”€â”€ dashboard/        # Dashboardç½‘é¡µ
â”œâ”€â”€ config/           # é…ç½®æ–‡ä»¶
â”œâ”€â”€ data/             # æ•°æ®ç›®å½•
â”œâ”€â”€ models/           # MLæ¨¡å‹
â”œâ”€â”€ reports/          # æŠ¥å‘Šè¾“å‡º
â””â”€â”€ docs/             # æ–‡æ¡£
```

### å¿«é€Ÿå¼€å§‹

1. é…ç½®ç¯å¢ƒ
2. ä¿®æ”¹config/config.json
3. è¿è¡Œæ•°æ®é‡‡é›†
4. è®­ç»ƒMLæ¨¡å‹
5. å¯åŠ¨Dashboard

## è”ç³»æ–¹å¼

GitHub: https://github.com/laobai6630-dotcom/cto/
'''

    ALL_FILE_TEMPLATES["docs/ARCHITECTURE.md"] = '''# ç³»ç»Ÿæ¶æ„

## æ•´ä½“æ¶æ„

å±±è„šä¸‹é¡¹ç›®é‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ï¼ŒåŒ…å«ä»¥ä¸‹æ ¸å¿ƒæ¨¡å—ï¼š

### 1. æ•°æ®é‡‡é›†æ¨¡å—

- æ—¥çº¿/åˆ†é’Ÿçº¿æ•°æ®
- å‘¨çº¿/æœˆçº¿æ•°æ®
- è´¢åŠ¡åŸºæœ¬é¢æ•°æ®

### 2. ç‰¹å¾å·¥ç¨‹æ¨¡å—

- 134ä¸ªåŸå§‹ç‰¹å¾æå–
- 10ä¸ªAIç‰¹å¾åˆæˆ
- 10ä¸ªç­¹ç ç‰¹å¾è®¡ç®—
- ç‰¹å¾æ ‡å‡†åŒ–ä¸é€‰æ‹©

### 3. å¯¹ç…§ç»„æ¨¡å—

- è¯†åˆ«è·Œå¹…å‰20å
- æå–å¯¹ç…§ç»„ç‰¹å¾
- å¯¹æ¯”åˆ†ç¦»åº¦åˆ†æ

### 4. MLè®­ç»ƒæ¨¡å—

- é€»è¾‘å›å½’æ¨¡å‹
- éšæœºæ£®æ—æ¨¡å‹
- æ¢¯åº¦æå‡æ¨¡å‹
- æ¨¡å‹é›†æˆ

### 5. ç­›é€‰æ¨¡å—

- ç›¸ä¼¼åº¦è®¡ç®—
- é€’è¿›ç­›é€‰é€»è¾‘

### 6. è·Ÿè¸ªæŠ¥å‘Šæ¨¡å—

- 30å¤©è·Ÿè¸ª
- æ—¥æŠ¥/å‘¨æŠ¥/æœˆæŠ¥

## æ•°æ®æµ

```
æ•°æ®é‡‡é›† â†’ ç‰¹å¾æå– â†’ MLæ¨¡å‹ â†’ ç›¸ä¼¼åº¦ç­›é€‰ â†’ å€™é€‰è‚¡ç¥¨ â†’ 30å¤©è·Ÿè¸ª
    â†“
å¯¹ç…§ç»„è¯†åˆ« â†’ å¯¹ç…§ç»„ç‰¹å¾ â†’ å¯¹æ¯”åˆ†æ
```
'''

    ALL_FILE_TEMPLATES["docs/API_REFERENCE.md"] = '''# APIå‚è€ƒ

## æ ¸å¿ƒAPI

### æ•°æ®é‡‡é›†API

```python
from scripts.data_collection.scheduler_main import DataCollectionScheduler

scheduler = DataCollectionScheduler()
scheduler.collect_daily_data()
```

### ç‰¹å¾æå–API

```python
from scripts.feature_engineering.feature_extraction import FeatureExtractor

extractor = FeatureExtractor(time_window=20)
features = extractor.extract_all_features('000001', datetime.now())
```

### MLè®­ç»ƒAPI

```python
from scripts.ml_training.model_training import ModelTrainer

trainer = ModelTrainer()
models = trainer.train_all_models()
```

### ç­›é€‰API

```python
from scripts.filtering.similarity_filter import SimilarityFilter

filter = SimilarityFilter()
results = filter.filter_with_multiple_thresholds([0.5, 0.4, 0.3])
```
'''

    ALL_FILE_TEMPLATES["docs/DEPLOYMENT.md"] = '''# éƒ¨ç½²æŒ‡å—

## ç¯å¢ƒè¦æ±‚

- Python 3.8+
- PostgreSQL (å¯é€‰)
- ä¾èµ–åŒ…è§requirements.txt

## å®‰è£…æ­¥éª¤

1. å…‹éš†ä»“åº“
```bash
git clone https://github.com/laobai6630-dotcom/cto.git
cd shanjiaxia_project
```

2. å®‰è£…ä¾èµ–
```bash
pip install -r requirements.txt
```

3. é…ç½®æ–‡ä»¶
```bash
cp config/config.json.example config/config.json
# ç¼–è¾‘config.jsonå¡«å…¥å¿…è¦é…ç½®
```

4. å¯åŠ¨æœåŠ¡
```bash
python scripts/data_collection/scheduler_main.py
```

## GitHub Actionsé…ç½®

é…ç½®è‡ªæ‰˜ç®¡Runnerä»¥åœ¨æœ¬åœ°æœåŠ¡å™¨ä¸Šè¿è¡Œå·¥ä½œæµ
'''

    ALL_FILE_TEMPLATES["docs/MAINTENANCE.md"] = '''# è¿ç»´æ‰‹å†Œ

## æ—¥å¸¸è¿ç»´

### 1. æ•°æ®å¤‡ä»½

- æ¯æ—¥è‡ªåŠ¨å¤‡ä»½åˆ°data/backup
- ä¿ç•™180å¤©å¤‡ä»½æ•°æ®
- å®šæœŸéªŒè¯å¤‡ä»½å®Œæ•´æ€§

### 2. æ—¥å¿—ç›‘æ§

- æ—¥å¿—ä½ç½®ï¼šlogs/
- å…³æ³¨ERRORå’ŒWARNINGçº§åˆ«
- å®šæœŸæ¸…ç†æ—§æ—¥å¿—

### 3. æ¨¡å‹æ›´æ–°

- å®šæœŸé‡æ–°è®­ç»ƒæ¨¡å‹
- è¯„ä¼°æ¨¡å‹æ€§èƒ½
- æ›´æ–°æ¨¡å‹æ–‡ä»¶

### 4. æ€§èƒ½ç›‘æ§

- ç›‘æ§Dashboardæ€§èƒ½
- ç›‘æ§æ•°æ®é‡‡é›†å»¶è¿Ÿ
- ç›‘æ§APIå“åº”æ—¶é—´

## æ•…éšœå¤„ç†

### å¸¸è§é—®é¢˜

1. æ•°æ®é‡‡é›†å¤±è´¥
   - æ£€æŸ¥APIè¿æ¥
   - æ£€æŸ¥ç½‘ç»œçŠ¶å†µ
   - æŸ¥çœ‹æ—¥å¿—è¯¦æƒ…

2. æ¨¡å‹é¢„æµ‹å¼‚å¸¸
   - æ£€æŸ¥ç‰¹å¾æ•°æ®
   - éªŒè¯æ¨¡å‹æ–‡ä»¶
   - é‡æ–°è®­ç»ƒæ¨¡å‹
'''

    ALL_FILE_TEMPLATES["docs/CHANGELOG.md"] = '''# ç‰ˆæœ¬æ—¥å¿—

## v2.0 (2025-12-19)

### æ–°å¢åŠŸèƒ½
- å¯¹ç…§ç»„åˆ†æåŠŸèƒ½
- ç­¹ç åˆ†å¸ƒç‰¹å¾
- AIç‰¹å¾åˆæˆ
- Dashboardå¯†ç ä¿æŠ¤
- ä¸­è‹±åŒè¯­æ”¯æŒ
- GitHubè‡ªåŠ¨åŒ–å·¥ä½œæµ
- ç›‘ç£æŠ¥å‘ŠåŠŸèƒ½

### æ”¹è¿›
- æ•°æ®ä¿ç•™æœŸå¢åŠ åˆ°180å¤©
- ç‰¹å¾æ—¶é—´çª—å£è°ƒæ•´ä¸º20å¤©
- ç‰¹å¾æ•°é‡å¢åŠ åˆ°88ä¸ª
- é€’è¿›ç­›é€‰é€»è¾‘ä¼˜åŒ–

### Bugä¿®å¤
- ä¿®å¤æ•°æ®æ¸…æ´—å¼‚å¸¸å€¼å¤„ç†
- ä¿®å¤ç‰¹å¾æ ‡å‡†åŒ–é—®é¢˜

## v1.0 (2025-11-01)

### åˆå§‹ç‰ˆæœ¬
- åŸºç¡€æ•°æ®é‡‡é›†åŠŸèƒ½
- 70ä¸ªç‰¹å¾æå–
- 3ä¸ªMLæ¨¡å‹è®­ç»ƒ
- ç®€å•ç­›é€‰é€»è¾‘
'''

    #####################################################################
    # æœ¬åœ°åŒ–æ–‡ä»¶
    #####################################################################
    
    ALL_FILE_TEMPLATES["locales/zh_CN.json"] = '''{
  "dashboard": {
    "title": "å±±è„šä¸‹é¡¹ç›®ç›‘æ§é¢æ¿",
    "candidates": "å€™é€‰è‚¡ç¥¨",
    "contrast_group": "å¯¹ç…§ç»„",
    "performance": "è¡¨ç°è¿½è¸ª",
    "settings": "å‚æ•°è®¾ç½®"
  },
  "buttons": {
    "login": "ç™»å½•",
    "logout": "ç™»å‡º",
    "save": "ä¿å­˜",
    "cancel": "å–æ¶ˆ",
    "refresh": "åˆ·æ–°"
  },
  "messages": {
    "success": "æ“ä½œæˆåŠŸ",
    "error": "æ“ä½œå¤±è´¥",
    "loading": "åŠ è½½ä¸­..."
  }
}
'''

    ALL_FILE_TEMPLATES["locales/en_US.json"] = '''{
  "dashboard": {
    "title": "Shanjiaxia Project Dashboard",
    "candidates": "Candidates",
    "contrast_group": "Contrast Group",
    "performance": "Performance Tracking",
    "settings": "Settings"
  },
  "buttons": {
    "login": "Login",
    "logout": "Logout",
    "save": "Save",
    "cancel": "Cancel",
    "refresh": "Refresh"
  },
  "messages": {
    "success": "Success",
    "error": "Error",
    "loading": "Loading..."
  }
}
'''

    #####################################################################
    # GitHub Workflows
    #####################################################################
    
    ALL_FILE_TEMPLATES[".github/workflows/daily.yml"] = '''name: Daily Analysis

on:
  schedule:
    - cron: '30 1 * * *'  # æ¯æ—¥09:30 (UTC+8)
  workflow_dispatch:

jobs:
  daily-analysis:
    runs-on: self-hosted
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.8'
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
      
      - name: Run daily analysis
        run: |
          python scripts/tracking/generate_daily_report.py
      
      - name: Commit results
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add reports/daily/
          git commit -m "Daily report $(date +%Y-%m-%d)" || true
          git push || true
'''

    ALL_FILE_TEMPLATES[".github/workflows/weekly.yml"] = '''name: Weekly Report

on:
  schedule:
    - cron: '0 8 * * 1'  # æ¯å‘¨ä¸€16:00 (UTC+8)
  workflow_dispatch:

jobs:
  weekly-report:
    runs-on: self-hosted
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Run weekly report
        run: |
          python scripts/tracking/generate_weekly_report.py
      
      - name: Commit results
        run: |
          git add reports/weekly/
          git commit -m "Weekly report" || true
          git push || true
'''

    ALL_FILE_TEMPLATES[".github/workflows/monthly.yml"] = '''name: Monthly Report

on:
  schedule:
    - cron: '0 8 1 * *'  # æ¯æœˆ1æ—¥16:00 (UTC+8)
  workflow_dispatch:

jobs:
  monthly-report:
    runs-on: self-hosted
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Run monthly report
        run: |
          python scripts/tracking/generate_monthly_report.py
      
      - name: Commit results
        run: |
          git add reports/monthly/
          git commit -m "Monthly report" || true
          git push || true
'''

    ALL_FILE_TEMPLATES[".github/workflows/trigger.yml"] = '''name: Manual Trigger

on:
  repository_dispatch:
    types: [manual-trigger]
  workflow_dispatch:

jobs:
  manual-trigger:
    runs-on: self-hosted
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Run trigger script
        run: |
          python scripts/github/github_trigger.py
'''

    ALL_FILE_TEMPLATES[".github/workflows/deploy.yml"] = '''name: Deploy Dashboard

on:
  push:
    branches: [ main ]
    paths:
      - 'dashboard/**'
  workflow_dispatch:

jobs:
  deploy:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Deploy to GitHub Pages
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./dashboard
'''

    #####################################################################
    # Dashboardæ–‡ä»¶ï¼ˆç®€åŒ–ç‰ˆï¼‰
    #####################################################################
    
    ALL_FILE_TEMPLATES["dashboard/index.html"] = '''<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>å±±è„šä¸‹é¡¹ç›® - Dashboard</title>
    <link rel="stylesheet" href="assets/css/styles.css">
</head>
<body>
    <div class="container">
        <header>
            <h1>å±±è„šä¸‹é¡¹ç›® v2.0 - ç›‘æ§é¢æ¿</h1>
            <div class="controls">
                <button id="loginBtn">ç™»å½•</button>
                <select id="langSelect">
                    <option value="zh_CN">ä¸­æ–‡</option>
                    <option value="en_US">English</option>
                </select>
            </div>
        </header>
        
        <main>
            <section id="overview">
                <h2>æ¦‚è§ˆ</h2>
                <div class="stats">
                    <div class="stat-card">
                        <h3>å€™é€‰è‚¡ç¥¨æ•°</h3>
                        <p id="candidateCount">-</p>
                    </div>
                    <div class="stat-card">
                        <h3>å¯¹ç…§ç»„è‚¡ç¥¨æ•°</h3>
                        <p id="contrastCount">20</p>
                    </div>
                    <div class="stat-card">
                        <h3>åˆ†ç¦»åº¦è¯„åˆ†</h3>
                        <p id="separationScore">-</p>
                    </div>
                </div>
            </section>
            
            <section id="candidates">
                <h2>å€™é€‰è‚¡ç¥¨åˆ—è¡¨</h2>
                <table id="candidatesTable">
                    <thead>
                        <tr>
                            <th>è‚¡ç¥¨ä»£ç </th>
                            <th>ç›¸ä¼¼åº¦</th>
                            <th>MLè¯„åˆ†</th>
                            <th>ç­¹ç è¯„åˆ†</th>
                        </tr>
                    </thead>
                    <tbody></tbody>
                </table>
            </section>
            
            <section id="parameters">
                <h2>å‚æ•°è®¾ç½®</h2>
                <div class="params">
                    <label>ç­›é€‰é˜ˆå€¼: <input type="number" id="thresholdInput" min="0" max="1" step="0.1" value="0.5"></label>
                    <button id="saveParamsBtn" disabled>ä¿å­˜å‚æ•°</button>
                </div>
            </section>
        </main>
    </div>
    
    <script src="assets/js/dashboard.js"></script>
    <script src="assets/js/auth.js"></script>
    <script src="assets/js/i18n.js"></script>
</body>
</html>
'''

    ALL_FILE_TEMPLATES["dashboard/assets/css/styles.css"] = '''/* å±±è„šä¸‹é¡¹ç›® Dashboard æ ·å¼ */

* {
    margin: 0;
    padding: 0;
    box-sizing: border-box;
}

body {
    font-family: 'Arial', sans-serif;
    background: #f5f5f5;
    color: #333;
}

.container {
    max-width: 1200px;
    margin: 0 auto;
    padding: 20px;
}

header {
    background: #2c3e50;
    color: white;
    padding: 20px;
    border-radius: 8px;
    margin-bottom: 20px;
    display: flex;
    justify-content: space-between;
    align-items: center;
}

header h1 {
    font-size: 24px;
}

.controls {
    display: flex;
    gap: 10px;
}

button {
    padding: 8px 16px;
    background: #3498db;
    color: white;
    border: none;
    border-radius: 4px;
    cursor: pointer;
}

button:hover {
    background: #2980b9;
}

button:disabled {
    background: #95a5a6;
    cursor: not-allowed;
}

.stats {
    display: grid;
    grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
    gap: 20px;
    margin-bottom: 20px;
}

.stat-card {
    background: white;
    padding: 20px;
    border-radius: 8px;
    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
}

.stat-card h3 {
    font-size: 14px;
    color: #7f8c8d;
    margin-bottom: 10px;
}

.stat-card p {
    font-size: 32px;
    font-weight: bold;
    color: #2c3e50;
}

section {
    background: white;
    padding: 20px;
    border-radius: 8px;
    margin-bottom: 20px;
    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
}

section h2 {
    margin-bottom: 15px;
    color: #2c3e50;
}

table {
    width: 100%;
    border-collapse: collapse;
}

thead {
    background: #ecf0f1;
}

th, td {
    padding: 12px;
    text-align: left;
    border-bottom: 1px solid #ddd;
}

.params {
    display: flex;
    gap: 15px;
    align-items: center;
}

input[type="number"] {
    padding: 8px;
    border: 1px solid #ddd;
    border-radius: 4px;
    width: 100px;
}
'''

    ALL_FILE_TEMPLATES["dashboard/assets/js/dashboard.js"] = '''// Dashboardä¸»é€»è¾‘

document.addEventListener('DOMContentLoaded', function() {
    loadOverviewData();
    loadCandidates();
    
    document.getElementById('saveParamsBtn').addEventListener('click', saveParameters);
});

function loadOverviewData() {
    // TODO: ä»APIåŠ è½½æ¦‚è§ˆæ•°æ®
    fetch('assets/data/overview.json')
        .then(response => response.json())
        .then(data => {
            document.getElementById('candidateCount').textContent = data.candidate_count || '-';
            document.getElementById('separationScore').textContent = data.separation_score ? data.separation_score.toFixed(4) : '-';
        })
        .catch(error => console.error('åŠ è½½æ¦‚è§ˆæ•°æ®å¤±è´¥:', error));
}

function loadCandidates() {
    // TODO: ä»APIåŠ è½½å€™é€‰è‚¡ç¥¨
    fetch('assets/data/candidates.json')
        .then(response => response.json())
        .then(data => {
            const tbody = document.querySelector('#candidatesTable tbody');
            tbody.innerHTML = '';
            
            data.forEach(candidate => {
                const row = tbody.insertRow();
                row.innerHTML = `
                    <td>${candidate.stock_code}</td>
                    <td>${candidate.similarity.toFixed(4)}</td>
                    <td>${candidate.ml_score.toFixed(4)}</td>
                    <td>${candidate.chip_score.toFixed(4)}</td>
                `;
            });
        })
        .catch(error => console.error('åŠ è½½å€™é€‰è‚¡ç¥¨å¤±è´¥:', error));
}

function saveParameters() {
    const threshold = document.getElementById('thresholdInput').value;
    
    // TODO: ä¿å­˜å‚æ•°åˆ°åç«¯
    console.log('ä¿å­˜å‚æ•°:', { threshold });
    alert('å‚æ•°å·²ä¿å­˜');
}
'''

    ALL_FILE_TEMPLATES["dashboard/assets/js/auth.js"] = '''// èº«ä»½éªŒè¯é€»è¾‘

let isAuthenticated = false;

document.getElementById('loginBtn').addEventListener('click', function() {
    if (isAuthenticated) {
        logout();
    } else {
        login();
    }
});

function login() {
    const password = prompt('è¯·è¾“å…¥å¯†ç :');
    
    // TODO: å®é™…å¯†ç éªŒè¯
    if (password === 'admin123') {
        isAuthenticated = true;
        document.getElementById('loginBtn').textContent = 'ç™»å‡º';
        document.getElementById('saveParamsBtn').disabled = false;
        alert('ç™»å½•æˆåŠŸï¼');
    } else {
        alert('å¯†ç é”™è¯¯ï¼');
    }
}

function logout() {
    isAuthenticated = false;
    document.getElementById('loginBtn').textContent = 'ç™»å½•';
    document.getElementById('saveParamsBtn').disabled = true;
    alert('å·²ç™»å‡º');
}
'''

    ALL_FILE_TEMPLATES["dashboard/assets/js/i18n.js"] = '''// å›½é™…åŒ–æ”¯æŒ

const translations = {
    zh_CN: null,
    en_US: null
};

let currentLang = 'zh_CN';

// åŠ è½½ç¿»è¯‘æ–‡ä»¶
fetch('../locales/zh_CN.json')
    .then(r => r.json())
    .then(data => translations.zh_CN = data);

fetch('../locales/en_US.json')
    .then(r => r.json())
    .then(data => translations.en_US = data);

document.getElementById('langSelect').addEventListener('change', function(e) {
    currentLang = e.target.value;
    updateLanguage();
});

function updateLanguage() {
    // TODO: æ›´æ–°é¡µé¢è¯­è¨€
    if (translations[currentLang]) {
        console.log('åˆ‡æ¢è¯­è¨€:', currentLang);
    }
}
'''

    ALL_FILE_TEMPLATES["dashboard/assets/data/overview.json"] = '''{
  "candidate_count": 15,
  "contrast_count": 20,
  "separation_score": 0.8234
}
'''

    ALL_FILE_TEMPLATES["dashboard/assets/data/candidates.json"] = '''[
  {
    "stock_code": "000001",
    "similarity": 0.8567,
    "ml_score": 0.8234,
    "chip_score": 0.7654
  },
  {
    "stock_code": "600000",
    "similarity": 0.8234,
    "ml_score": 0.7891,
    "chip_score": 0.7234
  }
]
'''

class ProjectGenerator:
    """é¡¹ç›®ä»£ç æ¡†æ¶ç”Ÿæˆå™¨"""
    
    def __init__(self, base_dir="shanjiaxia_project"):
        self.base_dir = Path(base_dir)
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.backup_dir = Path(f"backup_{self.timestamp}")
        print(f"åˆå§‹åŒ–é¡¹ç›®ç”Ÿæˆå™¨...")
        print(f"é¡¹ç›®ç›®å½•: {self.base_dir}")
        print(f"å¤‡ä»½ç›®å½•: {self.backup_dir}")
    
    def create_directory_structure(self):
        """åˆ›å»ºç›®å½•ç»“æ„"""
        directories = [
            "scripts/data_collection",
            "scripts/feature_engineering",
            "scripts/contrast_group",
            "scripts/ml_training",
            "scripts/filtering",
            "scripts/tracking",
            "scripts/github",
            "scripts/monitoring",
            "scripts/utils",
            "dashboard/assets/data",
            "dashboard/assets/css",
            "dashboard/assets/js",
            ".github/workflows",
            "config",
            "docs",
            "locales",
            "data/raw",
            "data/processed",
            "data/backup",
            "data/features",
            "data/candidates",
            "data/contrast_group",
            "data/analysis",
            "models",
            "logs",
            "reports/daily",
            "reports/weekly",
            "reports/monthly",
        ]
        
        print("\\nğŸ“ åˆ›å»ºç›®å½•ç»“æ„...")
        for directory in directories:
            dir_path = self.base_dir / directory
            dir_path.mkdir(parents=True, exist_ok=True)
        print(f"  âœ… å…±åˆ›å»º {len(directories)} ä¸ªç›®å½•")
    
    def write_file(self, filepath, content):
        """å†™å…¥æ–‡ä»¶"""
        full_path = self.base_dir / filepath
        full_path.parent.mkdir(parents=True, exist_ok=True)
        with open(full_path, 'w', encoding='utf-8') as f:
            f.write(content)
    
    def generate_all_files(self):
        """ç”Ÿæˆæ‰€æœ‰æ–‡ä»¶"""
        file_count = 0
        
        print("\\nğŸ“ ç”Ÿæˆæ‰€æœ‰é¡¹ç›®æ–‡ä»¶...")
        
        for filename, content in ALL_FILE_TEMPLATES.items():
            self.write_file(filename, content)
            file_count += 1
            print(f"  âœ… {filename}")
        
        return file_count
    
    def create_backup(self):
        """åˆ›å»ºå¤‡ä»½"""
        print("\\nğŸ“¦ åˆ›å»ºå¤‡ä»½...")
        if self.base_dir.exists():
            shutil.copytree(self.base_dir, self.backup_dir, dirs_exist_ok=True)
            print(f"  âœ… å¤‡ä»½å·²ä¿å­˜åˆ°: {self.backup_dir}")
    
    def run(self):
        """æ‰§è¡Œå®Œæ•´ç”Ÿæˆæµç¨‹"""
        print("=" * 70)
        print("ğŸš€ å±±è„šä¸‹é¡¹ç›® v2.0 - ä»£ç æ¡†æ¶ç”Ÿæˆå™¨")
        print("=" * 70)
        
        # 1. åˆ›å»ºç›®å½•ç»“æ„
        self.create_directory_structure()
        
        # 2. ç”Ÿæˆæ‰€æœ‰æ–‡ä»¶
        file_count = self.generate_all_files()
        
        # 3. åˆ›å»ºå¤‡ä»½
        self.create_backup()
        
        print("\\n" + "=" * 70)
        print(f"âœ… ä»£ç æ¡†æ¶ç”Ÿæˆå®Œæˆï¼å…±ç”Ÿæˆ {file_count} ä¸ªæ–‡ä»¶")
        print("=" * 70)
        print(f"\\nğŸ“‚ é¡¹ç›®ç›®å½•: {self.base_dir.absolute()}")
        print(f"ğŸ“¦ å¤‡ä»½ç›®å½•: {self.backup_dir.absolute()}")
        print("\\nä¸‹ä¸€æ­¥:")
        print(f"  1. cd {self.base_dir}")
        print("  2. æŸ¥çœ‹é¡¹ç›®ç»“æ„å’Œæ–‡ä»¶")
        print("  3. æ ¹æ®éœ€è¦è°ƒæ•´é…ç½®æ–‡ä»¶ï¼ˆconfig/ï¼‰")
        print("  4. å¼€å§‹å¼€å‘å’Œæµ‹è¯•å„æ¨¡å—")

if __name__ == '__main__':
    try:
        generator = ProjectGenerator()
        generator.run()
    except Exception as e:
        print(f"\\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
