#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å±±è„šä¸‹é¡¹ç›® v2.0 - å®Œæ•´ä»£ç æ¡†æ¶ç”Ÿæˆè„šæœ¬
Project: å±±è„šä¸‹ - Aè‚¡"å±±è„šä¸‹"å½¢æ€è‚¡ç¥¨ç²¾å‡†ç­›é€‰ç³»ç»Ÿ
Version: 2.0
Author: AI Agent
Date: 2025-12-19

ä½¿ç”¨æ–¹æ³•:
    python3 generate_all_files.py
    
è¿™ä¸ªè„šæœ¬å°†ç”Ÿæˆé¡¹ç›®çš„å®Œæ•´ç›®å½•ç»“æ„å’Œæ‰€æœ‰å¿…éœ€çš„æ–‡ä»¶ï¼ŒåŒ…æ‹¬:
- Pythonè„šæœ¬æ¨¡å—ï¼ˆæ•°æ®é‡‡é›†ã€ç‰¹å¾å·¥ç¨‹ã€MLè®­ç»ƒç­‰ï¼‰
- Dashboardç½‘é¡µï¼ˆHTML/CSS/JSï¼‰
- GitHub Workflows
- é…ç½®æ–‡ä»¶
- æ–‡æ¡£
- æœ¬åœ°åŒ–æ–‡ä»¶
"""

import os
import json
import shutil
from pathlib import Path
from datetime import datetime

class ProjectGenerator:
    """é¡¹ç›®ä»£ç æ¡†æ¶ç”Ÿæˆå™¨"""
    
    def __init__(self, base_dir="shanjiaxia_project"):
        self.base_dir = Path(base_dir)
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.backup_dir = Path(f"backup_{self.timestamp}")
        
        # å®šä¹‰æ‰€æœ‰æ–‡ä»¶æ¨¡æ¿
        self.file_templates = self.load_all_templates()
    
    def load_all_templates(self):
        """åŠ è½½æ‰€æœ‰æ–‡ä»¶æ¨¡æ¿"""
        templates = {}
        
        # åŠ è½½å„æ¨¡å—çš„æ¨¡æ¿
        templates.update(self.get_data_collection_templates())
        templates.update(self.get_feature_engineering_templates())
        templates.update(self.get_contrast_group_templates())
        templates.update(self.get_ml_training_templates())
        templates.update(self.get_filtering_templates())
        templates.update(self.get_tracking_templates())
        templates.update(self.get_github_templates())
        templates.update(self.get_monitoring_templates())
        templates.update(self.get_utils_templates())
        templates.update(self.get_dashboard_templates())
        templates.update(self.get_workflow_templates())
        templates.update(self.get_config_templates())
        templates.update(self.get_documentation_templates())
        templates.update(self.get_localization_templates())
        
        return templates
    
    def get_data_collection_templates(self):
        """æ•°æ®é‡‡é›†æ¨¡å—æ¨¡æ¿"""
        return {
            "scripts/data_collection/scheduler_main.py": """#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸»è°ƒåº¦ç¨‹åº - è´Ÿè´£è°ƒåº¦æ‰€æœ‰æ•°æ®é‡‡é›†ä»»åŠ¡
"""

import schedule
import time
import logging
from datetime import datetime
import json

logging.basicConfig(
    filename='logs/scheduler.log',
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

class DataCollectionScheduler:
    """æ•°æ®é‡‡é›†è°ƒåº¦å™¨"""
    
    def __init__(self, config_path='config/config.json'):
        self.config = self.load_config(config_path)
    
    def load_config(self, config_path):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            logging.error(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
            return {}
    
    def collect_daily_data(self):
        """é‡‡é›†æ—¥çº¿å’Œåˆ†é’Ÿçº¿æ•°æ®"""
        logging.info("å¼€å§‹é‡‡é›†æ—¥çº¿å’Œåˆ†é’Ÿçº¿æ•°æ®...")
        # TODO: å®ç°æ•°æ®é‡‡é›†é€»è¾‘
        
    def collect_weekly_data(self):
        """é‡‡é›†å‘¨çº¿æ•°æ®"""
        logging.info("å¼€å§‹é‡‡é›†å‘¨çº¿æ•°æ®...")
        # TODO: å®ç°å‘¨çº¿æ•°æ®é‡‡é›†é€»è¾‘
        
    def collect_monthly_data(self):
        """é‡‡é›†æœˆçº¿æ•°æ®"""
        logging.info("å¼€å§‹é‡‡é›†æœˆçº¿æ•°æ®...")
        # TODO: å®ç°æœˆçº¿æ•°æ®é‡‡é›†é€»è¾‘
        
    def collect_financial_data(self):
        """é‡‡é›†è´¢åŠ¡å’ŒåŸºæœ¬é¢æ•°æ®"""
        logging.info("å¼€å§‹é‡‡é›†è´¢åŠ¡å’ŒåŸºæœ¬é¢æ•°æ®...")
        # TODO: å®ç°è´¢åŠ¡æ•°æ®é‡‡é›†é€»è¾‘
    
    def setup_schedule(self):
        """è®¾ç½®å®šæ—¶ä»»åŠ¡"""
        schedule.every().day.at("09:00").do(self.collect_daily_data)
        schedule.every().monday.at("16:00").do(self.collect_weekly_data)
        schedule.every().day.at("10:00").do(self.collect_financial_data)
        logging.info("å®šæ—¶ä»»åŠ¡è®¾ç½®å®Œæˆ")
    
    def run(self):
        """è¿è¡Œè°ƒåº¦å™¨"""
        self.setup_schedule()
        logging.info("æ•°æ®é‡‡é›†è°ƒåº¦å™¨å¯åŠ¨")
        print("ğŸš€ æ•°æ®é‡‡é›†è°ƒåº¦å™¨å·²å¯åŠ¨...")
        
        while True:
            schedule.run_pending()
            time.sleep(60)

if __name__ == '__main__':
    scheduler = DataCollectionScheduler()
    scheduler.run()
""",

            "scripts/data_collection/data_cleaning.py": """#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®æ¸…æ´—æ¨¡å— - è´Ÿè´£æ•°æ®éªŒè¯ã€å¼‚å¸¸å€¼å¤„ç†ã€å­—æ®µæ ‡å‡†åŒ–
"""

import pandas as pd
import numpy as np
import logging
from pathlib import Path
from datetime import datetime, timedelta

logging.basicConfig(
    filename='logs/data_cleaning.log',
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

class DataCleaner:
    """æ•°æ®æ¸…æ´—å™¨"""
    
    def __init__(self, data_retention_days=180):
        self.data_retention_days = data_retention_days
        self.raw_data_dir = Path('data/raw')
        self.processed_data_dir = Path('data/processed')
        self.backup_dir = Path('data/backup')
        
        for directory in [self.raw_data_dir, self.processed_data_dir, self.backup_dir]:
            directory.mkdir(parents=True, exist_ok=True)
    
    def validate_data(self, df, required_columns):
        """éªŒè¯æ•°æ®å®Œæ•´æ€§"""
        missing_cols = set(required_columns) - set(df.columns)
        if missing_cols:
            raise ValueError(f"ç¼ºå°‘å¿…éœ€çš„åˆ—: {missing_cols}")
        
        missing_count = df.isnull().sum()
        if missing_count.any():
            logging.warning(f"å‘ç°ç¼ºå¤±å€¼:\n{missing_count[missing_count > 0]}")
        
        return True
    
    def handle_missing_values(self, df):
        """å¤„ç†ç¼ºå¤±å€¼"""
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        df[numeric_columns] = df[numeric_columns].fillna(method='ffill')
        df[numeric_columns] = df[numeric_columns].fillna(df[numeric_columns].mean())
        return df
    
    def remove_outliers(self, df, columns, n_std=3):
        """ç§»é™¤å¼‚å¸¸å€¼"""
        for col in columns:
            if col in df.columns and df[col].dtype in [np.float64, np.int64]:
                mean = df[col].mean()
                std = df[col].std()
                df = df[abs(df[col] - mean) <= n_std * std]
        return df
    
    def standardize_fields(self, df):
        """å­—æ®µæ ‡å‡†åŒ–"""
        if 'stock_code' in df.columns:
            df['stock_code'] = df['stock_code'].astype(str).str.zfill(6)
        
        if 'date' in df.columns:
            df['date'] = pd.to_datetime(df['date'])
        
        price_columns = ['open', 'high', 'low', 'close', 'adj_close']
        for col in price_columns:
            if col in df.columns:
                df[col] = df[col].round(2)
        
        return df
    
    def clean_daily_data(self, input_file='daily_data.csv'):
        """æ¸…æ´—æ—¥çº¿æ•°æ®"""
        try:
            df = pd.read_csv(self.raw_data_dir / input_file)
            
            required_columns = ['stock_code', 'date', 'open', 'high', 'low', 'close', 'volume']
            self.validate_data(df, required_columns)
            
            df = self.handle_missing_values(df)
            df = self.remove_outliers(df, ['open', 'high', 'low', 'close', 'volume'])
            df = self.standardize_fields(df)
            
            output_file = self.processed_data_dir / f"daily_data_cleaned_{datetime.now().strftime('%Y%m%d')}.csv"
            df.to_csv(output_file, index=False)
            
            logging.info(f"æ—¥çº¿æ•°æ®æ¸…æ´—å®Œæˆ: {output_file}")
            return df
        
        except Exception as e:
            logging.error(f"æ—¥çº¿æ•°æ®æ¸…æ´—å¤±è´¥: {str(e)}")
            raise
    
    def backup_historical_data(self):
        """å¤‡ä»½å†å²æ•°æ®ï¼ˆä¿ç•™180ä¸ªäº¤æ˜“æ—¥ï¼‰"""
        try:
            today = datetime.now().strftime('%Y-%m-%d')
            for data_file in self.processed_data_dir.glob('daily_data_cleaned_*.csv'):
                backup_file = self.backup_dir / f"daily_data_backup_{today}.csv"
                df = pd.read_csv(data_file)
                df.to_csv(backup_file, index=False)
                logging.info(f"åˆ›å»ºå¤‡ä»½: {backup_file}")
            
            cutoff_date = datetime.now() - timedelta(days=self.data_retention_days)
            for backup_file in self.backup_dir.glob('daily_data_backup_*.csv'):
                file_date_str = backup_file.stem.split('_')[-1]
                file_date = datetime.strptime(file_date_str, '%Y-%m-%d')
                if file_date < cutoff_date:
                    backup_file.unlink()
                    logging.info(f"åˆ é™¤è¿‡æœŸå¤‡ä»½: {backup_file}")
        
        except Exception as e:
            logging.error(f"å¤‡ä»½å¤±è´¥: {str(e)}")

if __name__ == '__main__':
    cleaner = DataCleaner(data_retention_days=180)
    cleaner.clean_daily_data()
    cleaner.backup_historical_data()
"""
        }
