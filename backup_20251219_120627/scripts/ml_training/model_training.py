#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨¡å‹è®­ç»ƒæ¨¡å— - è®­ç»ƒ3ä¸ªåŸºç¡€æ¨¡å‹
æ¨¡å‹: Logistic Regression, Random Forest, XGBoost/LightGBM
"""

import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb
from sklearn.model_selection import train_test_split
import joblib
from pathlib import Path
import logging

logging.basicConfig(
    filename='logs/ml_training.log',
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

class ModelTrainer:
    """æ¨¡å‹è®­ç»ƒå™¨ - è®­ç»ƒLRã€RFã€XGBä¸‰ä¸ªæ¨¡å‹"""
    
    def __init__(self):
        self.feature_dir = Path('data/features')
        self.contrast_dir = Path('data/contrast_group')
        self.model_dir = Path('models')
        self.model_dir.mkdir(parents=True, exist_ok=True)
    
    def load_training_data(self):
        """åŠ è½½è®­ç»ƒæ•°æ®ï¼ˆæˆåŠŸæ ·æœ¬ + å¯¹ç…§ç»„ï¼‰"""
        df_success = pd.read_csv(self.feature_dir / 'all_features_88.csv')
        df_success['label'] = 1
        
        df_contrast = pd.read_csv(self.contrast_dir / 'contrast_group_features_88.csv')
        df_contrast['label'] = 0
        
        df_train = pd.concat([df_success, df_contrast], ignore_index=True)
        
        logging.info(f"è®­ç»ƒæ•°æ®åŠ è½½å®Œæˆ: {len(df_train)}æ¡")
        return df_train
    
    def prepare_data(self, df):
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        X = df.drop(['stock_code', 'date', 'label'], axis=1)
        y = df['label']
        
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        return X_train, X_test, y_train, y_test
    
    def train_logistic_regression(self, X_train, y_train):
        """è®­ç»ƒé€»è¾‘å›å½’æ¨¡å‹"""
        logging.info("å¼€å§‹è®­ç»ƒé€»è¾‘å›å½’æ¨¡å‹...")
        
        model = LogisticRegression(
            max_iter=1000,
            random_state=42,
            class_weight='balanced'
        )
        model.fit(X_train, y_train)
        
        model_file = self.model_dir / 'model_lr.pkl'
        joblib.dump(model, model_file)
        
        logging.info(f"é€»è¾‘å›å½’æ¨¡å‹å·²ä¿å­˜: {model_file}")
        return model
    
    def train_random_forest(self, X_train, y_train):
        """è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹"""
        logging.info("å¼€å§‹è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹...")
        
        model = RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            random_state=42,
            class_weight='balanced',
            n_jobs=-1
        )
        model.fit(X_train, y_train)
        
        model_file = self.model_dir / 'model_rf.pkl'
        joblib.dump(model, model_file)
        
        logging.info(f"éšæœºæ£®æ—æ¨¡å‹å·²ä¿å­˜: {model_file}")
        return model
    
    def train_gradient_boosting(self, X_train, y_train):
        """è®­ç»ƒæ¢¯åº¦æå‡æ¨¡å‹"""
        logging.info(f"å¼€å§‹è®­ç»ƒæ¢¯åº¦æå‡æ¨¡å‹(XGBoost)...")
        
        model = xgb.XGBClassifier(
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            random_state=42,
            use_label_encoder=False,
            eval_metric='logloss'
        )
        
        model.fit(X_train, y_train)
        
        model_file = self.model_dir / f'model_gb_xgboost.pkl'
        joblib.dump(model, model_file)
        
        logging.info(f"æ¢¯åº¦æå‡æ¨¡å‹å·²ä¿å­˜: {model_file}")
        return model
    
    def train_all_models(self):
        """è®­ç»ƒæ‰€æœ‰3ä¸ªæ¨¡å‹"""
        df = self.load_training_data()
        X_train, X_test, y_train, y_test = self.prepare_data(df)
        
        model_lr = self.train_logistic_regression(X_train, y_train)
        model_rf = self.train_random_forest(X_train, y_train)
        model_gb = self.train_gradient_boosting(X_train, y_train)
        
        test_data = {
            'X_test': X_test,
            'y_test': y_test
        }
        joblib.dump(test_data, self.model_dir / 'test_data.pkl')
        
        logging.info("æ‰€æœ‰æ¨¡å‹è®­ç»ƒå®Œæˆ")
        
        print("\nâœ… æ¨¡å‹è®­ç»ƒå®Œæˆ:")
        print("  ğŸ“Š é€»è¾‘å›å½’: model_lr.pkl")
        print("  ğŸŒ² éšæœºæ£®æ—: model_rf.pkl")
        print("  ğŸš€ æ¢¯åº¦æå‡: model_gb_xgboost.pkl")
        
        return {
            'lr': model_lr,
            'rf': model_rf,
            'gb': model_gb
        }

if __name__ == '__main__':
    trainer = ModelTrainer()
    models = trainer.train_all_models()
